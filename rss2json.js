const rss2json = {"Scitech": {"https://newatlas.com/science/index.rss": {"feed": {"title": "Science"}, "entries": []}, "https://www.cbsnews.com/latest/rss/science": {"feed": {"title": "Science - CBSNews.com"}, "entries": [{"title": "Cracking the code: Using genetic genealogy to unmask serial criminals", "summary": "Barbara Rae-Venter never anticipated that her genealogy hobby would lead to the capture of one of California's most notorious criminals, the Golden State Killer. Her pioneering use of genetic genealogy has since helped solve numerous cold cases.", "link": "https://www.cbsnews.com/news/cracking-the-code-using-genetic-genealogy-to-unmask-serial-criminals/", "published_js": "2025-07-13", "author": "None"}]}, "https://www.cbsnews.com/latest/rss/space": {"feed": {"title": "Space - CBSNews.com"}, "entries": []}}, "Gadgets": {"https://www.emergentmind.com/feeds/rss": {"feed": {"title": "Emergent Mind Feed"}, "entries": []}}, "Food_Health": {"https://phys.org/rss-feed/biology-news/agriculture/": {"feed": {"title": "Agriculture news"}, "entries": [{"title": "Healthier cows, creamier milk: How stem cells could revolutionize dairy farming", "summary": "Modern dairy cows are milk-producing powerhouses\u2014some yielding more than 50 kg a day. But all that productivity comes at a price: their mammary glands often suffer from inflammation and cellular stress, which not only reduces milk quality but also affects the cows' well-being.", "link": "https://phys.org/news/2025-07-healthier-cows-creamier-stem-cells.html", "published_js": "2025-07-14", "author": "None"}, {"title": "How plants survive drought: The unsuspected role of myosin XI in guard cells", "summary": "With intensifying global warming and climate change, drought has become a major threat to global agriculture, impacting crop yields and food security. To survive such adverse events, plants have evolved several strategies. One such strategy to counteract water scarcity is \"stomatal closure,\" where stomata\u2014the tiny...", "link": "https://phys.org/news/2025-07-survive-drought-unsuspected-role-myosin.html", "published_js": "2025-07-14", "author": "None"}, {"title": "Tackling a $1.2 billion problem for Australian farmers using dirt-cheap crushed volcanic rock", "summary": "Cheap volcanic rock that languishes in open-cut mines and quarries could transform Australia's farming sector as a natural fertilizer, boosting crop yields and removing carbon dioxide from the atmosphere.", "link": "https://phys.org/news/2025-07-tackling-billion-problem-australian-farmers.html", "published_js": "2025-07-14", "author": "None"}]}}, "Nature": {"http://feeds.feedburner.com/DiscoverLivingWorld": {"feed": {"title": "Planet Earth | Discover Magazine"}, "entries": [{"title": "The Way Parasitoid Wasps Lay Their Eggs Has Inspired Horror Films", "summary": "Learn about the wasps that have a gruesome but fascinating way of surviving.", "link": "https://www.discovermagazine.com/planet-earth/the-way-parasitoid-wasps-lay-their-eggs-has-inspired-horror-films", "published_js": "2025-07-14", "author": "Avery Hurt"}]}, "http://feeds.feedburner.com/DiscoverEnvironment": {"feed": {"title": "Environment | Discover Magazine"}, "entries": []}}, "Business": {}, "Foss_Self-hosting": {}, "History": {}, "News": {"https://www.livemint.com/rss/politics/": {"feed": {"title": "mint - politics"}, "entries": [{"title": "Statistics and Politics Are\u00a0a Dangerous\u00a0Mix", "summary": "There are\u00a0two threats currently facing government data agencies: inadequate funding and political interference.", "link": "https://www.livemint.com/politics/statistics-and-politics-are-a-dangerousmix-11752495903997.html", "published_js": "2025-07-14", "author": "None"}, {"title": "South Africa Aims to Forge G-20 Accord Despite Ongoing US Enmity", "summary": "South Africa expressed confidence that it can get finance ministers and central bank governors from the Group of 20 nations to agree on a communique for the first time since it assumed the leadership of the bloc, despite differences between the US and its other...", "link": "https://www.livemint.com/politics/south-africa-aims-to-forge-g-20-accord-despite-ongoing-us-enmity-11752495656736.html", "published_js": "2025-07-14", "author": "None"}, {"title": "Explained: What is \u2018Martyrs\u2019 Day\u2019 in Kashmir that Omar Abdullah compares with 'Jallianwala Bagh' massacre?", "summary": "The commemoration of July 13, once an official holiday in Jammu and Kashmir to honour martyrs from 1931, has ignited a political firestorm. With recent detentions of political leaders, the day symbolises deep-rooted tensions in the region post-Article 370 abrogation.", "link": "https://www.livemint.com/politics/news/explained-what-is-july-13-martyrs-day-why-has-it-triggered-a-political-storm-in-jammu-and-kashmir-11752475264571.html", "published_js": "2025-07-14", "author": "None"}, {"title": "Albanese Reaffirms Taiwan Stance as He Starts China Visit", "summary": "Australia opposes any unilateral moves to change the Taiwan Strait status quo, Prime Minister Anthony Albanese said as he began a trip to China to maintain steady ties with his country\u2019s top trading partner.", "link": "https://www.livemint.com/politics/news/albanese-reaffirms-taiwan-stance-as-he-starts-china-visit-11752465886609.html", "published_js": "2025-07-14", "author": "None"}, {"title": "India should avoid hasty trade deal under US pressure, as it may not survive next US political shift: GTRI", "summary": "India must stay firm and avoid trading away its core sectors, especially agriculture, despite increasing pressure from the United States under President Donald Trump's administration, according to a report by the Global Trade Research Initiative (GTRI).", "link": "https://www.livemint.com/politics/news/india-should-avoid-hasty-trade-deal-under-us-pressure-as-it-may-not-survive-next-us-political-shift-gtri-11752460971870.html", "published_js": "2025-07-14", "author": "None"}, {"title": "RSS not a pressure group, but a facilitator: Ram Madhav on Sangh's ties with BJP", "summary": "Senior BJP leader Ram Madhav clarifies the RSS's role as a facilitator rather than a pressuring force in party decisions, especially regarding the upcoming election of a new national president amidst speculation and political manoeuvring.", "link": "https://www.livemint.com/politics/news/rss-not-a-pressure-group-but-a-facilitator-ram-madhav-amid-delay-in-electing-bjp-president-pick-11752458036024.html", "published_js": "2025-07-14", "author": "None"}]}, "https://www.livemint.com/rss/industry": {"feed": {"title": "mint - industry"}, "entries": [{"title": "Brands fall head over heels for <i>Kyunki Saas Bhi Kabhi Bahu Thi </i>reboot", "summary": "Advertising slots for the new iteration of Kyunki Saas Bhi Kabhi Bahu Thi are selling for  \u20b93.5 lakh-4.5 lakh per 10 seconds, and commanding a 75% premium over Star Plus\u2019 long-running, leading show Anupamaa.", "link": "https://www.livemint.com/industry/media/brands-fall-head-over-heels-for-ekta-kapoors-smriti-irani-starrer-kyunki-saas-bhi-kabhi-bahu-thi-reboot-11752491421480.html", "published_js": "2025-07-14", "author": "None"}, {"title": "Margins pressure, credit costs may plague banks in Q1; asset quality seen largely stable", "summary": "Q1FY26 is expected to be a muted quarter with most banks juggling with many adversely moving aspects such as slowing advances growth, margin pressure from consecutive repo rate cuts, said analysts", "link": "https://www.livemint.com/industry/banking/banks-q1-earnings-bad-loans-asset-quality-bank-credit-deposit-ratio-rbi-hdfc-indusind-11752484796648.html", "published_js": "2025-07-14", "author": "None"}, {"title": "We need a revamped Incredible India campaign to lift tourism: HAI's MP Bezbaruah", "summary": "India's tourism sector is experiencing a robust recovery, with 9.23 million foreign tourist arrivals in 2023, up from 6.44 million in 2022. The government aims to reach 100 million tourists by 2027, with significant growth in domestic travel and increased infrastructure investment needed.", "link": "https://www.livemint.com/industry/we-need-a-revamped-incredible-india-campaign-to-lift-tourism-hais-mp-bezbaruah-11752489769801.html", "published_js": "2025-07-14", "author": "None"}, {"title": "Single-window licensing portal for cab aggregators may go live by year-end", "summary": "A unified process of accepting and issuing licences would allow companies such as Ola, Uber and Rapido to enter new markets more quickly, supporting growth and innovation.", "link": "https://www.livemint.com/industry/cab-aggregator-ola-uber-rapido-cab-licence-portal-india-online-portal-for-cab-aggregators-11752481438131.html", "published_js": "2025-07-14", "author": "None"}, {"title": "Box office blues force brands to rethink Bollywood bets", "summary": "Brands are shifting their endorsement strategies due to the inconsistent performance of Bollywood films, opting for south Indian stars, influencers, and sports icons for better ROI and cultural relevance.", "link": "https://www.livemint.com/industry/media/dwindling-bollywood-box-office-has-made-brands-cautious-and-stars-may-bear-the-brunt-11752396864393.html", "published_js": "2025-07-14", "author": "None"}, {"title": "Centre\u2019s schemes to make micro and small businesses green find no takers: Less than 1% of funds disbursed", "summary": "Only 629 GIFT applications and four SPICE applications have been approved till June, data on the Union MSME ministry\u2019s performance smartboard showed.", "link": "https://www.livemint.com/industry/micro-small-businesses-msme-gift-spice-green-investments-exports-interest-subvention-scheme-loan-disbursal-11752238986356.html", "published_js": "2025-07-14", "author": "None"}, {"title": "Bank holiday today: Are banks closed today, on July 14, on account of Beh Deinkhlam festival celebrations?", "summary": "Bank holiday today: Are banks in Meghalaya closed today, on July 14, for Beh Deinkhlam celebrations?", "link": "https://www.livemint.com/industry/banking/bank-holiday-today-are-banks-meghalaya-closed-today-july-14-beh-deinkhlam-festival-celebration-jaintia-tribe-sbi-india-11752456045852.html", "published_js": "2025-07-14", "author": "None"}, {"title": "Govt eases exhaust gas rules for thermal power plants, may lower power prices", "summary": "Thermal power plants outside a 10km radius of cities with a population of 1 million people or more will be excluded from the requirement to install FGD systems.", "link": "https://www.livemint.com/industry/energy/coalbased-power-plants-fgd-systems-flue-gas-desulphurization-electricity-prices-sulphur-dioxide-so2-emissions-norms-11752420747370.html", "published_js": "2025-07-13", "author": "None"}]}, "swarajyamag.com": {"feed": {"title": "swarajyamag.com"}, "entries": []}}, "Reddit": {"https://oauth.reddit.com/r/todayilearned/top": {"feed": {"title": "Reddit - TIL"}, "entries": [{"title": "TIL in 2020 a man in Como, Italy stepped outside to cool off after fighting with his wife and ended up walking 450km. His walk eventually ended a week later when he was stopped in Fano and fined \u20ac400 for breaking the curfew. His wife, who had reported him missing, travelled to Fano to collect him.", "summary": "", "link": "https://www.bbc.com/news/world-europe-55224031", "author": "None"}, {"title": "TIL that due to teasing, basketball player God Shammgod went by Shammgod Wells until high school. He only reverted to his birth name when he enrolled in college as he was told he would have to register under his legal name and could not afford to have it legally changed.", "summary": "", "link": "https://sports.yahoo.com/blogs/ncaab-jim-weber/god-shammgods-unforgettable-name-still-bringing-him-fame.html?guccounter=1&amp;guce_referrer=aHR0cHM6Ly9lbi5tLndpa2lwZWRpYS5vcmcv&amp;guce_referrer_sig=AQAAALKHvxw1HRAc8q2vnraD3N_jVyu7XbxGdSy1wFBeuLvalKjx-xkik0R7Lwd8C4RgMZp76cNgRfCKHHJ-FAMsVZc36zjlxrL9fiduDwXehNM43forph1_H8tibtN6aW6j2lJJ9cCAa5JYspVB5Z8i9wFHDyDEQZcToySwXscBl5D1", "author": "None"}, {"title": "TIL Prince Charles &amp; Princess Diana only met in person 13x before getting engaged and when they were asked if they were in love, Charles said \"whatever in love means\". Then on the night before their wedding, he reportedly told her that he didn't love her in order to get everything out in the open.", "summary": "", "link": "https://people.com/prince-charles-princess-diana-relationship-timeline-5894506", "author": "None"}, {"title": "TIL nobody is entirely sure why the beverage 7UP was named 7UP", "summary": "", "link": "https://www.soda-fountain.com/p/untangling-the-lie-of-the-7up-origin", "author": "None"}, {"title": "TIL 5 Kyoto temples have bloodstained ceilings taken from Fushimi Castle floorboards, site of a siege &amp; mass suicide that delayed Ishida Mitsunari\u2019s forces. This allowed Tokugawa leyasu to prepare for the battle of Sekigahara, unifying Japan. The ceilings honor the fallen samurai of Fushumi Castle.", "summary": "", "link": "https://burialsandbeyond.com/2023/04/22/the-bloody-ceilings-of-kyoto-japan/", "author": "None"}, {"title": "TIL Kansai International Airport in Japan averages 20-30 million passengers a year and it has not lost a single piece of luggage since opening in 1994.", "summary": "", "link": "https://www.cnn.com/travel/japan-kansai-airport-no-lost-luggage-intl-hnk", "author": "None"}, {"title": "TIL the name of the Australian outback town Coober Pedy, where almost everyone lives underground, translates to whitefellas hole in Aboriginal", "summary": "", "link": "https://en.wikipedia.org/wiki/Coober_Pedy?wprov=sfti1", "author": "None"}, {"title": "TIL the tradition of white wedding dresses were started by Queen Victoria. in 1840 Before then brides used their best dress of any color, even black ones.", "summary": "", "link": "https://en.wikipedia.org/wiki/Wedding_dress_of_Queen_Victoria", "author": "None"}, {"title": "TIL that when the Crown Prince of Albania, Leka returned to Albania in 2022, he brought with him 11 cases of automatic weapons, grenades, and hunting arms. The authorities seized them but gave them back after being deemed items of cultural heritage.", "summary": "", "link": "https://en.wikipedia.org/wiki/Leka,_Crown_Prince_of_Albania#Personal_life", "author": "None"}, {"title": "TIL actor Johnny Lewis killed his 81 year old landlady and her cat, attacked neighbors, then died from a fall. No drugs or alcohol were found in his system. His death was ruled accidental, and some believe untreated head trauma led to his violent behavior.", "summary": "", "link": "https://www.wikipedia.org/wiki/Johnny_Lewis", "author": "None"}, {"title": "TIL Ron Gilbert, co-developer of the 1987 game \"Maniac Mansion,\" coined the phrase \"cutscenes\" for the game's innovative use of non-playable videos that \"interrupt gameplay to advance the story and inform the player about offscreen events.\"", "summary": "", "link": "https://en.wikipedia.org/wiki/Maniac_Mansion", "author": "None"}, {"title": "TIL that Sylvester Stallone\u2019s famous look is due to a nerve injury at birth, not Bell\u2019s Palsy", "summary": "", "link": "https://flipthemoviescript.com/sylvester-stallone-face-is-result-of-facial-nerve-killing-at-birth-not-bells-palsy/", "author": "None"}, {"title": "TIL about the Mirror Test, a method for determining whether a non-human animal has the ability of self-recognition when looking into a mirror. Elephants, chimpanzees, orangutans, gorillas, dolphins and manta rays have successfully passed the test.", "summary": "", "link": "https://www.animalcognition.org/2015/04/15/list-of-animals-that-have-passed-the-mirror-test/", "author": "None"}, {"title": "TIL Citibank accidentally paid $893 million to Revlon's lenders in 2020, instead of $7.8 million that they meant to, mostly due to bad U.I.", "summary": "", "link": "https://arstechnica.com/tech-policy/2021/02/citibank-just-got-a-500-million-lesson-in-the-importance-of-ui-design/", "author": "None"}, {"title": "TIL diamond has exceptional heat conductivity (better than copper)", "summary": "", "link": "https://www.coherent.com/news/blog/diamond-heat-spreaders#:~:text=Diamond%20is%20an%20exceptional%20conductor%20of%20heat%2C,W/mK)%20and%20aluminum%20(up%20to%20220%20W/mK).", "author": "None"}, {"title": "TIL 20 billion pounds of produce are thrown out in the US every year", "summary": "", "link": "https://shapiroe.com/blog/ugly-fruit-vegetable-food-waste/?utm_source=chatgpt.com", "author": "None"}, {"title": "TIL that your taste buds have a lifespan or around 10 to 14 days &amp; your body is constantly replacing them", "summary": "", "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC5178033/", "author": "None"}, {"title": "TIL U.S. Senator Daniel Sickles was acquitted of murder in 1859 after killing his wife\u2019s lover\u2014by using the first successful temporary insanity defence in U.S. legal history.", "summary": "", "link": "https://en.wikipedia.org/wiki/Daniel_Sickles", "author": "None"}, {"title": "TIL That Vanilla beans are the product of the world's only fruit-producing orchid, the Vanilla planifolia", "summary": "", "link": "https://en.wikipedia.org/wiki/Vanilla", "author": "None"}, {"title": "TIL the oldest human footprints ever found outside Africa are nearly 1 million years old, and were discovered on a beach in Norfolk, England.", "summary": "", "link": "https://en.wikipedia.org/wiki/Happisburgh_footprints?v2", "author": "None"}, {"title": "TIL Josef Goebbels wrote a three-part semi-autobiographical novel called \u201cMichael: A German Destiny in Diary Form\u201d which was published in 1929. The story is about a young man returning to Weimar Germany after serving in World War I.", "summary": "", "link": "https://en.wikipedia.org/wiki/Michael_(novel)", "author": "None"}, {"title": "TIL that Cat Stevens released an \u201celectro\u201d-style instrumental in 1977 called \u201cWas Dog a Doughnut\u201d. The track sounded very different from his earlier work, and was widely sampled in the early hip-hop scene. The title parodies an article published around that time, titled \u201cWas God an Astronaut?\u201d", "summary": "", "link": "https://faroutmagazine.co.uk/was-dog-a-doughnut-how-cat-stevens-accidentally-had-a-huge-influence-on-hip-hop-with-his-bizarre-electro-venture/", "author": "None"}, {"title": "TIL about the concept \"Mise an abyme\" which is  is a term borrowed from heraldry and later adopted in literary, artistic, and media studies to describe a technique where a work contains a smaller version of itself, often recursively like a painting within a painting.", "summary": "", "link": "https://en.wikipedia.org/wiki/Mise_en_abyme", "author": "None"}, {"title": "TIL of Irelands only native reptile, the Common Lizard (Lacerta Vivipara)", "summary": "", "link": "https://irelandswildlife.com/common-lizard-lacerta-vivipara/", "author": "None"}]}, "https://oauth.reddit.com/.json": {"feed": {"title": "Reddit - TIL"}, "entries": [{"title": "FAA says Boeing fuel switches are safe following fatal Air India crash", "summary": "", "link": "https://www.ft.com/content/64de9574-a236-439c-a6a8-2a472444e615", "author": "None"}, {"title": "India's Jaishankar hails positive trajectory in China ties \u2013 DW", "summary": "", "link": "https://www.dw.com/en/india-china-call-for-stronger-cooperation-during-jaishankars-beijing-visit/a-73265940", "author": "None"}, {"title": "India tops global fast payments race, UPI processes over 18 billion transactions per month: IMF report", "summary": "", "link": "https://www.livemint.com/news/india-tops-global-fast-payments-race-upi-processes-over-18-billion-transactions-per-month-imf-report-11752163944164.html", "author": "None"}, {"title": "[Bug] When fine-tuning Qwen3 , an 'deallocating None'error occurs after few minutes: Conflict Between Gradient Checkpointing and Memory Management", "summary": "1. Did you update? pip install --upgrade unsloth unsloth_zoo  **yes**\n2. Colab or Kaggle or local / cloud  **cloud**\n3. Number GPUs used, use nvidia-smi  **1 RTX4090 24GB**\n4. Which notebook? Please link! \n**but replace the 14B model with 8B**\n6. Which Unsloth version, TRL version, transformers version, PyTorch version? \n**Unsloth: 2025.7.3\nTRL: 0.19.1.\ntransformer version: 4.53.2.\npytorch version: 2.7.1+cu126.**\n8. Which trainer? SFTTrainer, GRPOTrainer **SFTTrainer**\n## Here is the code (\n\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nfrom unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Qwen3-8B\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number &gt; 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)\n\nalpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs       = examples[\"input\"]\n    outputs      = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\npass\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\ndataset = dataset.map(formatting_prompts_func, batched = True,)\n\nfrom trl import SFTConfig, SFTTrainer\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    args = SFTConfig(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        # Use num_train_epochs = 1, warmup_ratio for full training runs!\n        warmup_ratio = 0.05,\n        num_train_epochs = 1,\n        learning_rate = 2e-4,\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n        report_to = \"none\", # Use this for WandB etc\n    ),\n)\n\ntrainer_stats = trainer.train()\n\nprint(f\"peak VRAM during training: {torch.cuda.max_memory_allocated() / (1024**3):.2f} GB\")\n\n## The 'deallocating None' error\n\n\ud83e\udda5 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n\ud83e\udda5 Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.7.3: Fast Qwen3 patching. Transformers: 4.53.2.\n   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.546 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.1\n\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n \"-____-\"     Free license: \nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:02&lt;00:00,  1.08s/it]\nUnsloth 2025.7.3 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 51,760 | Num Epochs = 1 | Total steps = 6,470\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n \"-____-\"     Trainable parameters = 43,646,976 of 8,234,382,336 (0.53% trained)\n  0%|                                                                                                                                                  | 0/6470 [00:00&lt;?, ?it/s]Unsloth: Will smartly offload gradients to save VRAM!\n{'loss': 1.5335, 'grad_norm': 1.1586451530456543, 'learning_rate': 0.0, 'epoch': 0.0}                                                                                           \n{'loss': 1.8746, 'grad_norm': 1.9488970041275024, 'learning_rate': 6.17283950617284e-07, 'epoch': 0.0}                                                                          \n{'loss': 1.6318, 'grad_norm': 1.0615123510360718, 'learning_rate': 1.234567901234568e-06, 'epoch': 0.0}                                                                         \n{'loss': 1.9605, 'grad_norm': 1.4692251682281494, 'learning_rate': 1.8518518518518519e-06, 'epoch': 0.0}                                                                        \n{'loss': 1.7414, 'grad_norm': 1.3316459655761719, 'learning_rate': 2.469135802469136e-06, 'epoch': 0.0}                                                                         \n{'loss': 1.6718, 'grad_norm': 1.2041643857955933, 'learning_rate': 3.0864197530864196e-06, 'epoch': 0.0}                                                                        \n{'loss': 1.3887, 'grad_norm': 1.1421422958374023, 'learning_rate': 3.7037037037037037e-06, 'epoch': 0.0}                                                                        \n{'loss': 1.7128, 'grad_norm': 1.130318284034729, 'learning_rate': 4.3209876543209875e-06, 'epoch': 0.0}                                                                         \n{'loss': 1.6933, 'grad_norm': 1.3437644243240356, 'learning_rate': 4.938271604938272e-06, 'epoch': 0.0}                                                                         \n{'loss': 1.816, 'grad_norm': 1.6011966466903687, 'learning_rate': 5.555555555555556e-06, 'epoch': 0.0}                                                                          \n{'loss': 1.4728, 'grad_norm': 1.2972931861877441, 'learning_rate': 6.172839506172839e-06, 'epoch': 0.0}                                                                         \n{'loss': 1.4726, 'grad_norm': 0.9943879246711731, 'learning_rate': 6.790123456790123e-06, 'epoch': 0.0}                                                                         \n{'loss': 1.5535, 'grad_norm': 1.375585913658142, 'learning_rate': 7.4074074074074075e-06, 'epoch': 0.0}                                                                         \n{'loss': 1.5928, 'grad_norm': 1.1027742624282837, 'learning_rate': 8.02469135802469e-06, 'epoch': 0.0}                                                                          \n{'loss': 1.6504, 'grad_norm': 1.7101731300354004, 'learning_rate': 8.641975308641975e-06, 'epoch': 0.0}                                                                         \n{'loss': 1.3699, 'grad_norm': 1.1548311710357666, 'learning_rate': 9.259259259259259e-06, 'epoch': 0.0}                                                                         \n{'loss': 1.4848, 'grad_norm': 1.0099883079528809, 'learning_rate': 9.876543209876543e-06, 'epoch': 0.0}                                                                         \n{'loss': 1.8883, 'grad_norm': 1.093531847000122, 'learning_rate': 1.0493827160493827e-05, 'epoch': 0.0}                                                                         \n{'loss': 1.5092, 'grad_norm': 1.1205849647521973, 'learning_rate': 1.1111111111111112e-05, 'epoch': 0.0}                                                                        \n{'loss': 1.3454, 'grad_norm': 1.0613555908203125, 'learning_rate': 1.1728395061728396e-05, 'epoch': 0.0}                                                                        \n{'loss': 1.6567, 'grad_norm': 1.7389315366744995, 'learning_rate': 1.2345679012345678e-05, 'epoch': 0.0}                                                                        \n{'loss': 1.7274, 'grad_norm': 1.7506530284881592, 'learning_rate': 1.2962962962962962e-05, 'epoch': 0.0}                                                                        \n{'loss': 1.5671, 'grad_norm': 1.3537321090698242, 'learning_rate': 1.3580246913580247e-05, 'epoch': 0.0}                                                                        \n{'loss': 1.5943, 'grad_norm': 1.2660235166549683, 'learning_rate': 1.419753086419753e-05, 'epoch': 0.0}                                                                         \n{'loss': 1.7, 'grad_norm': 1.4568794965744019, 'learning_rate': 1.4814814814814815e-05, 'epoch': 0.0}                                                                           \n{'loss': 1.3861, 'grad_norm': 0.6871325969696045, 'learning_rate': 1.54320987654321e-05, 'epoch': 0.0}                                                                          \n{'loss': 1.458, 'grad_norm': 0.6980249285697937, 'learning_rate': 1.604938271604938e-05, 'epoch': 0.0}                                                                          \n{'loss': 1.3204, 'grad_norm': 0.5967793464660645, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.0}                                                                        \n{'loss': 1.493, 'grad_norm': 0.9154291749000549, 'learning_rate': 1.728395061728395e-05, 'epoch': 0.0}                                                                          \n{'loss': 1.2161, 'grad_norm': 0.6217581629753113, 'learning_rate': 1.7901234567901236e-05, 'epoch': 0.0}                                                                        \n{'loss': 1.1898, 'grad_norm': 0.4963208734989166, 'learning_rate': 1.8518518518518518e-05, 'epoch': 0.0}                                                                        \n{'loss': 1.3331, 'grad_norm': 0.6608074307441711, 'learning_rate': 1.91358024691358e-05, 'epoch': 0.0}                                                                          \n{'loss': 1.3632, 'grad_norm': 0.5628055930137634, 'learning_rate': 1.9753086419753087e-05, 'epoch': 0.01}                                                                       \n{'loss': 1.5375, 'grad_norm': 0.9648422598838806, 'learning_rate': 2.037037037037037e-05, 'epoch': 0.01}                                                                        \n{'loss': 1.3623, 'grad_norm': 0.7103092074394226, 'learning_rate': 2.0987654320987655e-05, 'epoch': 0.01}                                                                       \n{'loss': 1.1643, 'grad_norm': 0.520149827003479, 'learning_rate': 2.1604938271604937e-05, 'epoch': 0.01}                                                                        \n{'loss': 1.1316, 'grad_norm': 0.4760976731777191, 'learning_rate': 2.2222222222222223e-05, 'epoch': 0.01}                                                                       \n{'loss': 1.2334, 'grad_norm': 0.7474365830421448, 'learning_rate': 2.2839506172839506e-05, 'epoch': 0.01}                                                                       \n{'loss': 1.3911, 'grad_norm': 0.5614683628082275, 'learning_rate': 2.345679012345679e-05, 'epoch': 0.01}                                                                        \n{'loss': 1.574, 'grad_norm': 0.5633246302604675, 'learning_rate': 2.4074074074074074e-05, 'epoch': 0.01}                                                                        \n{'loss': 1.2766, 'grad_norm': 0.5257001519203186, 'learning_rate': 2.4691358024691357e-05, 'epoch': 0.01}                                                                       \n{'loss': 1.257, 'grad_norm': 0.3717462122440338, 'learning_rate': 2.5308641975308646e-05, 'epoch': 0.01}                                                                        \n{'loss': 1.2297, 'grad_norm': 0.5548499226570129, 'learning_rate': 2.5925925925925925e-05, 'epoch': 0.01}                                                                       \n{'loss': 1.1637, 'grad_norm': 0.4260367751121521, 'learning_rate': 2.654320987654321e-05, 'epoch': 0.01}                                                                        \n{'loss': 1.306, 'grad_norm': 0.46264535188674927, 'learning_rate': 2.7160493827160493e-05, 'epoch': 0.01}                                                                       \n{'loss': 1.1819, 'grad_norm': 0.3945801556110382, 'learning_rate': 2.777777777777778e-05, 'epoch': 0.01}                                                                        \n{'loss': 1.0657, 'grad_norm': 0.5817477107048035, 'learning_rate': 2.839506172839506e-05, 'epoch': 0.01}                                                                        \n{'loss': 1.514, 'grad_norm': 0.426167756319046, 'learning_rate': 2.9012345679012347e-05, 'epoch': 0.01}                                                                         \n{'loss': 1.1059, 'grad_norm': 0.4089460074901581, 'learning_rate': 2.962962962962963e-05, 'epoch': 0.01}                                                                        \n{'loss': 1.2627, 'grad_norm': 0.3137648105621338, 'learning_rate': 3.0246913580246916e-05, 'epoch': 0.01}                                                                       \n{'loss': 1.2759, 'grad_norm': 0.3695306181907654, 'learning_rate': 3.08641975308642e-05, 'epoch': 0.01}                                                                         \n{'loss': 1.1175, 'grad_norm': 0.409766286611557, 'learning_rate': 3.148148148148148e-05, 'epoch': 0.01}                                                                         \n{'loss': 1.2249, 'grad_norm': 0.41780900955200195, 'learning_rate': 3.209876543209876e-05, 'epoch': 0.01}                                                                       \n{'loss': 1.287, 'grad_norm': 0.29309114813804626, 'learning_rate': 3.271604938271605e-05, 'epoch': 0.01}                                                                        \n{'loss': 0.9236, 'grad_norm': 0.2527065873146057, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.01}                                                                       \n{'loss': 1.1535, 'grad_norm': 0.2348678559064865, 'learning_rate': 3.395061728395062e-05, 'epoch': 0.01}                                                                        \n{'loss': 1.0127, 'grad_norm': 0.28041112422943115, 'learning_rate': 3.45679012345679e-05, 'epoch': 0.01}                                                                        \n{'loss': 0.8609, 'grad_norm': 0.2403581440448761, 'learning_rate': 3.518518518518519e-05, 'epoch': 0.01}                                                                        \n{'loss': 0.9689, 'grad_norm': 0.2739495635032654, 'learning_rate': 3.580246913580247e-05, 'epoch': 0.01}                                                                        \n{'loss': 1.0284, 'grad_norm': 0.251027375459671, 'learning_rate': 3.6419753086419754e-05, 'epoch': 0.01}                                                                        \n{'loss': 1.0106, 'grad_norm': 0.2457178384065628, 'learning_rate': 3.7037037037037037e-05, 'epoch': 0.01}                                                                       \n{'loss': 1.1357, 'grad_norm': 0.3444538414478302, 'learning_rate': 3.7654320987654326e-05, 'epoch': 0.01}                                                                       \n{'loss': 1.1207, 'grad_norm': 0.3194916248321533, 'learning_rate': 3.82716049382716e-05, 'epoch': 0.01}                                                                         \n{'loss': 1.0885, 'grad_norm': 0.3959096670150757, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.01}                                                                        \n{'loss': 0.8973, 'grad_norm': 0.224856436252594, 'learning_rate': 3.950617283950617e-05, 'epoch': 0.01}                                                                         \n{'loss': 1.0292, 'grad_norm': 0.2687690556049347, 'learning_rate': 4.012345679012346e-05, 'epoch': 0.01}                                                                        \n{'loss': 1.2321, 'grad_norm': 0.26913684606552124, 'learning_rate': 4.074074074074074e-05, 'epoch': 0.01}                                                                       \n{'loss': 1.0354, 'grad_norm': 0.3219553828239441, 'learning_rate': 4.135802469135803e-05, 'epoch': 0.01}                                                                        \n{'loss': 1.0956, 'grad_norm': 0.2424125075340271, 'learning_rate': 4.197530864197531e-05, 'epoch': 0.01}                                                                        \n{'loss': 0.9071, 'grad_norm': 0.1958129107952118, 'learning_rate': 4.259259259259259e-05, 'epoch': 0.01}                                                                        \n{'loss': 0.9949, 'grad_norm': 0.27624988555908203, 'learning_rate': 4.3209876543209875e-05, 'epoch': 0.01}                                                                      \n{'loss': 1.19, 'grad_norm': 0.32887527346611023, 'learning_rate': 4.3827160493827164e-05, 'epoch': 0.01}                                                                        \n{'loss': 0.8387, 'grad_norm': 0.39763182401657104, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.01}                                                                      \n{'loss': 0.9759, 'grad_norm': 0.3532586693763733, 'learning_rate': 4.506172839506173e-05, 'epoch': 0.01}                                                                        \n{'loss': 1.0312, 'grad_norm': 0.42153316736221313, 'learning_rate': 4.567901234567901e-05, 'epoch': 0.01}                                                                       \n{'loss': 0.854, 'grad_norm': 0.3147733509540558, 'learning_rate': 4.62962962962963e-05, 'epoch': 0.01}                                                                          \n{'loss': 0.7429, 'grad_norm': 0.254463255405426, 'learning_rate': 4.691358024691358e-05, 'epoch': 0.01}                                                                         \n{'loss': 0.9262, 'grad_norm': 0.18668106198310852, 'learning_rate': 4.7530864197530866e-05, 'epoch': 0.01}                                                                      \n{'loss': 0.9376, 'grad_norm': 0.2754688858985901, 'learning_rate': 4.814814814814815e-05, 'epoch': 0.01}                                                                        \n{'loss': 1.1589, 'grad_norm': 0.23302432894706726, 'learning_rate': 4.876543209876544e-05, 'epoch': 0.01}                                                                       \n{'loss': 0.961, 'grad_norm': 0.17880386114120483, 'learning_rate': 4.938271604938271e-05, 'epoch': 0.01}                                                                        \n{'loss': 0.8139, 'grad_norm': 0.2941263020038605, 'learning_rate': 5e-05, 'epoch': 0.01}                                                                                        \n{'loss': 0.892, 'grad_norm': 0.21924927830696106, 'learning_rate': 5.061728395061729e-05, 'epoch': 0.01}                                                                        \n{'loss': 1.0589, 'grad_norm': 0.2704322934150696, 'learning_rate': 5.1234567901234574e-05, 'epoch': 0.01}                                                                       \n{'loss': 1.0676, 'grad_norm': 0.23829656839370728, 'learning_rate': 5.185185185185185e-05, 'epoch': 0.01}                                                                       \n{'loss': 0.891, 'grad_norm': 0.18838883936405182, 'learning_rate': 5.246913580246914e-05, 'epoch': 0.01}                                                                        \n{'loss': 0.9467, 'grad_norm': 0.22593863308429718, 'learning_rate': 5.308641975308642e-05, 'epoch': 0.01}                                                                       \n  1%|\u2588\u258a                                                                                                                                     | 87/6470 [01:53&lt;2:27:02,  1.38s/it]Fatal Python error: none_dealloc: deallocating None\nPython runtime state: initialized\n\nThread 0x00007fe5aaf33640 (most recent call first):\n  File \"/home/panzhizhen/miniconda3/envs/unsloth/lib/python3.10/threading.py\", line 324 in wait\n  File \"/home/panzhizhen/miniconda3/envs/unsloth/lib/python3.10/threading.py\", line 607 in wait\n  File \"/home/panzhizhen/miniconda3/envs/unsloth/lib/python3.10/site-packages/tqdm/_monitor.py\", line 60 in run\n  File \"/home/panzhizhen/miniconda3/envs/unsloth/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n  File \"/home/panzhizhen/miniconda3/envs/unsloth/lib/python3.10/threading.py\", line 973 in _bootstrap\n\nCurrent thread 0x00007fe6e36ff640 (most recent call first):\n  &lt;no Python frame&gt;\n\nThread 0x00007fe6e97a2640 (most recent call first):\n  File \"/home/panzhizhen/miniconda3/envs/unsloth/lib/python3.10/threading.py\", line 324 in wait\n  File \"/home/panzhizhen/miniconda3/envs/unsloth/lib/python3.10/threading.py\", line 607 in wait\n  File \"/home/panzhizhen/miniconda3/envs/unsloth/lib/python3.10/site-packages/tqdm/_monitor.py\", line 60 in run\n  File \"/home/panzhizhen/miniconda3/envs/unsloth/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n  File \"/home/panzhizhen/miniconda3/envs/unsloth/lib/python3.10/threading.py\", line 973 in _bootstrap\n\nThread 0x00007fe71dfff640 (most recent call first):\n  File \"/home/panzhizhen/miniconda3/envs/unsloth/lib/python3.10/threading.py\", line 324 in wait\n  File \"/home/panzhizhen/miniconda3/envs/unsloth/lib/python3.10/threading.py\", line 607 in wait\n  File \"/home/panzhizhen/miniconda3/envs/unsloth/lib/python3.10/site-packages/tqdm/_monitor.py\", line 60 in run\n  File \"/home/panzhizhen/miniconda3/envs/unsloth/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n  File \"/home/panzhizhen/miniconda3/envs/unsloth/lib/python3.10/threading.py\", line 973 in _bootstrap\n\nThread 0x00007fe74d197640 (most recent call first):\n  File \"/home/panzhizhen/miniconda3/envs/unsloth/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 55 in _recv_msg\n  File \"/home/panzhizhen/miniconda3/envs/unsloth/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 191 in _read_thread\n  File \"/home/panzhizhen/miniconda3/envs/unsloth/lib/python3.10/threading.py\", line 953 in run\n  File \"/home/panzhizhen/miniconda3/envs/unsloth/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n  File \"/home/panzhizhen/miniconda3/envs/unsloth/lib/python3.10/threading.py\", line 973 in _bootstrap\n\nThread 0x00007fe998c65740 (most recent call first):\n  File \"/home/panzhizhen/miniconda3/envs/unsloth/lib/python3.10/site-packages/torch/autograd/graph.py\", line 824 in _engine_run_backward\n  File \"/home/panzhizhen/miniconda3/envs/unsloth/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 353 in backward\n  File \"/home/panzhizhen/miniconda3/envs/unsloth/lib/python3.10/site-packages/torch/_tensor.py\", line 648 in backward\n  File \"/home/panzhizhen/miniconda3/envs/unsloth/lib/python3.10/site-packages/accelerate/accelerator.py\", line 2553 in backward\n  File \"&lt;string&gt;\", line 82 in _unsloth_training_step\n  File \"/home/panzhizhen/Projects/unsloth/unsloth/AblationExperiments/unsloth_compiled_cache/UnslothSFTTrainer.py\", line 896 in training_step\n  File \"&lt;string&gt;\", line 323 in _fast_inner_training_loop\n  File \"/home/panzhizhen/miniconda3/envs/unsloth/lib/python3.10/site-packages/transformers/trainer.py\", line 2206 in train\n  File \"/home/panzhizhen/Projects/unsloth/unsloth/AblationExperiments/Unsloth_alpaca.py\", line 88 in &lt;module&gt;\n", "link": "https://www.reddit.com/r/unsloth/comments/1lzhtqk/bug_when_finetuning_qwen3_an_deallocating/", "author": "None"}, {"title": "A military exercise drawing together 19 nations and 35,000 forces begins in Australia", "summary": "", "link": "https://www.asahi.com/ajw/articles/15904686", "author": "None"}, {"title": "Australia will not commit troops in advance to any conflict, minister says", "summary": "", "link": "https://www.reuters.com/world/china/australia-will-not-commit-troops-advance-any-conflict-minister-says-2025-07-13/", "author": "None"}, {"title": "I\u2019m an ex-Googler \u2014 we built an AI voice agent that answers calls, books leads, and fixes a huge gap in service businesses", "summary": "I used to lead Google Ads and AI projects during my 9+ years at Google. After leaving, I started a performance agency focused on law firms and home service businesses.\n\nWe were crushing it on the lead gen side but clients were still losing money because no one was answering the phone.\n\nThat pain led us to build Donna: an out-of-the-box AI voice assistant that picks up every call, handles intake, books appointments, and even processes cancellations. No call centers. No missed leads. It just works.\n\n\n\nSome early lessons from 1000+ calls:\n\u2022 Most leads are lost after the ad click\n\u2022 After-hours responsiveness = major revenue unlock\n\u2022 AI voice can work extremely well when it\u2019s vertical-specific\n\u2022 SMBs don\u2019t need dashboards they need outcomes\n\nCurious if anyone else here has tackled this \u201clead leakage\u201d problem, or is building similar vertical AI tools.\n", "link": "https://www.reddit.com/r/speechtech/comments/1lzbk94/im_an_exgoogler_we_built_an_ai_voice_agent_that/", "author": "None"}, {"title": "The chip race: US opens tech gates for China; Should Indian companies worry? - Times of India", "summary": "", "link": "https://timesofindia.indiatimes.com/business/india-business/the-chip-race-us-opens-tech-gates-for-china-should-indian-companies-worry/articleshow/122418369.cms", "author": "None"}, {"title": "Can Australia, India and Indonesia really sail together?", "summary": "", "link": "https://www.lowyinstitute.org/the-interpreter/can-australia-india-indonesia-really-sail-together", "author": "None"}, {"title": "India should view China with greater rationality, less politicization and sensationalism", "summary": "", "link": "https://www.globaltimes.cn/page/202507/1337922.shtml", "author": "None"}, {"title": "profullstack/launchpadder-web: The platform enables frictionless URL submission with automated metadata scraping, AI-generated descriptions, and federated submissions across multiple directories.", "summary": "", "link": "https://github.com/profullstack/launchpadder-web", "author": "None"}, {"title": "NLC India in advanced talks with Russian firm for sourcing lithium", "summary": "", "link": "https://economictimes.indiatimes.com/industry/indl-goods/svs/metals-mining/nlc-india-in-advanced-talks-with-russian-firm-for-sourcing-lithium/articleshow/122414815.cms?from=mdr", "author": "None"}, {"title": "Excited to share updates to Open WebUI Starter! New docs, Docker support, and templates for everyone", "summary": "Hey everyone! I\u2019m thrilled to share some exciting updates to my GitHub project, [Open WebUI Starter]( Over the last few weeks, I\u2019ve been focused on making this tool more accessible, flexible, and transparent for users. Here\u2019s what\u2019s new:\n\n# \ud83e\uddf1 Improved Documentation &amp; Structure\n\nI\u2019ve completely overhauled the documentation to make it easier to understand and navigate. The project is now split into two repositories to streamline workflows:\n\n* [**Open WebUI Starter App** ]( A bash script that lets you **create, remove, start, stop, and view** your OWUI environment. Great for quick setups!\n* [**Open WebUI Starter Templates** ]( A repository for customized OWUI installations. Think of it as a \"template library\" where you can tailor your setup to your needs.\n\n# \ud83e\uddea Docker Compose Support\n\nThe starter app uses Docker Compose under the hood, making it easier to manage dependencies and configurations. Less manual setup\u2014just run a few commands and you\u2019re up and running!\n\n# \ud83d\udee0\ufe0f Collaboration Welcome\n\nI\u2019m working on a list of pre-built templates to help users get started faster. If you\u2019re interested in contributing a template, helping with documentation, or brainstorming ideas, **let me know!** This is a community project, and I want to make sure it\u2019s as useful as possible for everyone.\n\n# \ud83e\udde9 What\u2019s Next?\n\n* More pre-built templates for common use cases (e.g., LLMs, RAG, etc.)\n* Better command-line interface (CLI) tooling for managing environments\n* A \"starter kit\" for beginners\n\n# \ud83d\ude80 How to Get Started\n\n1. Check out the [starter app repo ]( a quick start.\n2. Explore the [templates repo ]( customizations.\n3. Reach out with ideas or feedback\u2014this is a collaborative effort!\n\n**P.S.** Want to chat about the project or collaborate? DM me or reply here!", "link": "https://www.reddit.com/r/OpenWebUI/comments/1lzb8z7/excited_to_share_updates_to_open_webui_starter/", "author": "None"}, {"title": "PSA: R/Archeology is run by Nazi apologists.", "summary": "The subject of Nazis and the Wehrmacht came up on r/archeology the other day and users began to post about how they believe the Wehrmacht wasn't responsible for war crimes and thus weren't Nazis and only the SS were really Nazis. This could not be further from the truth, it is literal Nazi propaganda and the war crimes of Nazi Germany would not have been possible without the actions of the men in the Wehrmacht. When users attempted to point this out, one of the mod team began to act like holding the Wehrmacht accountable is political extremism and that the myth of the clean Wehrmacht is actually the nuanced academic position when again that could not be further from the truth. I reported the mod for spreading Nazi propaganda, but the mod team muted me without response. so either the rest of the mod team is OK with Nazi propaganda or are inactive. either way, that means that sub is run by Nazis or Nazi apologists who want to use the guise of moderating an \"academic sub\" as a smokescreen for spreading Nazi propaganda.\n\n\nthat sub already has a problem with encouraging looting, but spreading Nazi propaganda is a bridge too far and that sub should be avoided.\n\nEDIT: another mod has spoken up, so I'll go ahead and single out the mod that was not only speaking out in support of nazi apologia but using mod powers to do so, it was Bo-zard.", "link": "https://www.reddit.com/r/Archaeology/comments/1lzdjcg/psa_rarcheology_is_run_by_nazi_apologists/", "author": "None"}, {"title": "Why a U.S.-India Trade Deal Makes Sense?", "summary": "", "link": "https://www.cfr.org/article/why-us-india-trade-deal-makes-sense", "author": "None"}, {"title": "Never rode in a carriage, never got promoted, never died in battle: the incredible life of the foot soldier who served 75 years under three kings and said \u201cno\u201d to Napoleon", "summary": "", "link": "https://peakd.com/history/@arraymedia/never-rode-in-a-carriage-never-got-promoted-never-died-in-battle-the-incredible-life-of-the-foot-soldier-who-served-75-years-un", "author": "None"}, {"title": "Anyone know what features are cooking up for Open WebUI 0.6.16?", "summary": "It\u2019s been a minute since 0.6.15 dropped. I\u2019ve been following this project since the early days, and this seems like the longest stretch I can remember between releasees. I\u2019m guessing either Tim and the contributor team are taking some much deserved time off, or there\u2019s some serious cooking going on right now. Either way, I love this project and I\u2019m excited to see what\u2019s in store for 0.6.16 and beyond. Every release seems to make an already great project better. \nAny particular feature you are hoping is in the upcoming release? ", "link": "https://www.reddit.com/r/OpenWebUI/comments/1lzbwhd/anyone_know_what_features_are_cooking_up_for_open/", "author": "None"}, {"title": "The end of hasbara as we know it: A proactive doctrine for prevailing in the modern information war", "summary": "", "link": "https://www.ynetnews.com/opinions-analysis/article/b1vcj7e8xe", "author": "None"}, {"title": "TUI for Alias Management with Command Usage Tracking and Smart alias suggestions", "summary": "Hey everyone,\n\nI built\u00a0**alman (alias manager)**\u00a0a command-line tool and TUI designed to make alias management easier, by using a cool algorithm to detect commands in your terminal workflow which could benefit from having an alias, and then intelligently suggesting an alias for that command, thereby saving you time and keystrokes.\n\nHere is the\u00a0**github**\u00a0:\u00a0[\n\n**Alman ranking algorithm**\n\nAlman ranks your commands based on:\n\n* **Length**: Longer commands get a slight boost (using length\\^(3/5) to avoid bias).\n* **Frequency**: Commands you use often score higher.\n* **Last use time**: Recent commands get a multiplier (e.g., 4x for &lt;1 hour, 2x for &lt;1 day, 0.5x for &lt;1 week, 0.25x for older).\n\nThis ensures the most useful commands are prioritized for alias creation. It then generates intelligent alias suggestions using schemes like:\n\n* **Vowel Removal**: git status \u2192 gst\n* **Abbreviation**: ls -la \u2192 ll\n* **First Letter Combination**: docker compose \u2192 dcompose\n* **Smart Truncation**: git checkout \u2192 gco\n* **Prefix Matching**: git commands \u2192 g + subcommand letter\n\nSome of its features are:\n\n* Interactive aliases for browsing adding and removing aliases.\n* Ability to\u00a0**track your aliases across multiple shells and multiple alias files**.\n* Command-line mode for quick alias operations.\n* Cross-platform: Works on Linux, macOS, BSD, and Windows (via WSL).\n\nAlman offers an installation script that works on any platform for easy setup and is also available through cargo, yay, etc.\n\nTry it out and streamline your workflow. I\u2019d really appreciate any feedback or suggestions, and if you find it helpful, feel free to check it out and star the repo.", "link": "https://github.com/vaibhav-mattoo/alman", "author": "None"}, {"title": "8 Khalistani terrorists arrested in US in gang case, NIA most wanted among them", "summary": "", "link": "https://www.indiatoday.in/india/story/khalistani-terrorists-wanted-by-national-investigation-agency-arrested-in-us-in-gang-related-case-2754927-2025-07-12", "author": "None"}, {"title": "The 4-Layer Framework for Building Context-Proof AI Prompts", "summary": "You spend hours perfecting a prompt that works flawlessly in one scenario. Then you try it elsewhere and it completely falls apart.\n\nI've tested thousands of prompts across different AI models, conversation lengths, and use cases. Unreliable prompts usually fail for predictable reasons. Here's a framework that dramatically improved my prompt consistency.\n\n# The Problem with Most Prompts\n\nMost prompts are built like houses of cards. They work great until something shifts. Common failure points:\n\n* Works in short conversations but breaks in long ones\n* Perfect with GPT-4 but terrible with Claude\n* Great for your specific use case but useless for teammates\n* Performs well in English but fails in other languages\n\n# The 4-Layer Reliability Framework\n\n# Layer 1: Core Instruction Architecture\n\nStart with bulletproof structure:\n\n    ROLE: [Who the AI should be]\n    TASK: [What exactly you want done]\n    CONTEXT: [Essential background info]\n    CONSTRAINTS: [Clear boundaries and rules]\n    OUTPUT: [Specific format requirements]\n    \n\nThis skeleton works across every AI model I've tested. Make each section explicit rather than assuming the AI will figure it out.\n\n# Layer 2: Context Independence\n\nMake your prompt work regardless of conversation history:\n\n* **Always restate key information** \\- don't rely on what was said 20 messages ago\n* **Define terms within the prompt** \\- \"By analysis I mean...\"\n* **Include relevant examples** \\- show don't just tell\n* **Set explicit boundaries** \\- \"Only consider information provided in this prompt\"\n\n# Layer 3: Model-Agnostic Language\n\nDifferent AI models have different strengths. Use language that works everywhere:\n\n* **Avoid model-specific tricks** \\- that Claude markdown hack won't work in GPT\n* **Use clear, direct language** \\- skip the \"act as if you're Shakespeare\" stuff\n* **Be specific about reasoning** \\- \"Think step by step\" works better than \"be creative\"\n* **Test with multiple models** \\- what works in one fails in another\n\n# Layer 4: Failure-Resistant Design\n\nBuild in safeguards for when things go wrong:\n\n* **Include fallback instructions** \\- \"If you cannot determine X, then do Y\"\n* **Add verification steps** \\- \"Before providing your answer, check if...\"\n* **Handle edge cases explicitly** \\- \"If the input is unclear, ask for clarification\"\n* **Provide escape hatches** \\- \"If this task seems impossible, explain why\"\n\n# Real Example: Before vs After\n\n**Before (Unreliable):** \"Write a professional email about the meeting\"\n\n**After (Reliable):**\n\n    ROLE: Professional business email writer\n    TASK: Write a follow-up email for a team meeting\n    CONTEXT: Meeting discussed Q4 goals, budget concerns, and next steps\n    CONSTRAINTS: \n    - Keep under 200 words\n    - Professional but friendly tone\n    - Include specific action items\n    - If meeting details are unclear, ask for clarification\n    OUTPUT: Subject line + email body in standard business format\n    \n\n# Testing Your Prompts\n\nHere's my reliability checklist:\n\n1. **Cross-model test** \\- Try it in at least 2 different AI systems\n2. **Conversation length test** \\- Use it early and late in long conversations\n3. **Context switching test** \\- Use it after discussing unrelated topics\n4. **Edge case test** \\- Try it with incomplete or confusing inputs\n5. **Teammate test** \\- Have someone else use it without explanation\n\nQuick note on organization: If you're building a library of reliable prompts, track which ones actually work consistently. You can organize them in [Notion]( [Obsidian]( or even a simple spreadsheet. I personally do it in [EchoStash]( which I find more convenient. The key is having a system to test and refine your prompts over time.\n\n# The 10-Minute Rule\n\nSpend 10 minutes stress-testing every prompt you plan to reuse. It's way faster than debugging failures later.\n\nThe goal isn't just prompts that work. It's prompts that work reliably, every time, regardless of context.\n\n**What's your biggest prompt reliability challenge? I'm curious what breaks most often for others.**", "link": "https://www.reddit.com/r/PromptEngineering/comments/1lzfbw6/the_4layer_framework_for_building_contextproof_ai/", "author": "None"}, {"title": "Code Prompt Enhancer For Vscode, Cursor, Windsurf", "summary": "This project provides a Python application that enhances code prompts using the Groq API. It includes a GUI for easy interaction and hotkey support for quick enhancement of selected text.", "link": "https://github.com/naijagamerx/code-prompt-enhancer", "author": "None"}, {"title": "Prompt engineering isn\u2019t about clever wording It\u2019s about clear thinking.", "summary": "\nI\u2019ve found the best results come when I treat the AI like a junior dev: give it structure, context, and a clear goal. A solid system (like a plan.md or task checklist) works better than any fancy phrasing.\n\nWould love to hear how others approach prompting for large codebases or multi-step tasks.\n", "link": "https://www.reddit.com/r/PromptEngineering/comments/1lziq31/prompt_engineering_isnt_about_clever_wording_its/", "author": "None"}, {"title": "Visually impaired 10-year-old rocks Stevie Wonder at fundraiser, receives $25K in surprise donations", "summary": "I know this happened back in February, but I just stumbled across it and couldn\u2019t not share it here.\nAt a fundraiser earlier this year, 10-year-old Grayson Roberts, who\u2019s visually impaired, jumped on the drums and played Stevie Wonder\u2019s Superstition for a room full of people. He and his mom had recently lost their home in Altadena, CA.\nThe crowd was so moved that donations started coming in. Chuck D and Flavor Flav gave $5,000 on the spot, and Lou Taylor (CEO of Tri Star) later donated $20,000 to the family.\nJust one of those moments where people came together for the right reasons. I love this kid\u2019s courage and spirit.\n", "link": "https://variety.com/2025/music/news/public-enemy-andra-day-anthony-hamilton-black-music-action-coalition-fundraiser-1236291418/", "author": "None"}, {"title": "NY Times rejects Netanyahu\u2019s denial of report he prolonged Gaza war to stay in power | \"The statement from the Prime Minister\u2019s office does not refute the facts of that reporting.\"", "summary": "", "link": "https://www.timesofisrael.com/ny-times-rejects-netanyahus-denial-of-report-he-prolonged-gaza-war-to-stay-in-power/", "author": "None"}]}}, "Sport": {}};