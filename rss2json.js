const rss2json = {"Scitech": {"https://newatlas.com/science/index.rss": {"feed": {"title": "Science"}, "entries": [{"title": "This chip uses light to supercharge AI and cut energy use", "summary": "Convolutional neural networks, or CNNs, are the workhorses behind many of AI's greatest hits, like spotting faces in photos, reading handwriting, or translating languages. They're masters at pattern recognition, scanning raw data with tiny filters (called kernels) to pick out meaningful features, kind of like...", "link": "https://newatlas.com/materials/light-based-chip-ai-power-efficiency/", "published_js": "2025-10-02", "author": "Pranjal Malewar"}]}, "https://www.cbsnews.com/latest/rss/science": {"feed": {"title": "Science - CBSNews.com"}, "entries": [{"title": "Saturn's moon shows favorable conditions for life: \"Simply phenomenal\"", "summary": "Enceladus has long been considered a prime candidate in the search for life beyond Earth because of its hidden ocean and plumes of water erupting from cracks near its south pole.", "link": "https://www.cbsnews.com/news/saturn-moon-enceladus-favorable-conditions-life-study/", "published_js": "2025-10-02", "author": "None"}, {"title": "From the archives: Naturalist Jane Goodall", "summary": "Famed naturalist Jane Goodall, who dedicated her life to studying chimpanzees and protecting the environment, died on Wednesday, Oct. 1, 2025 at age 91. In this Oct. 24, 2021 \"Sunday Morning\" profile, she talked with Seth Doane about her fascination with animals, her groundbreaking work...", "link": "https://www.cbsnews.com/video/from-the-archives-naturalist-jane-goodall/", "published_js": "2025-10-01", "author": "None"}]}, "https://www.cbsnews.com/latest/rss/space": {"feed": {"title": "Space - CBSNews.com"}, "entries": []}}, "Gadgets": {"https://www.emergentmind.com/feeds/rss": {"feed": {"title": "Emergent Mind Feed"}, "entries": []}}, "Food_Health": {"https://phys.org/rss-feed/biology-news/agriculture/": {"feed": {"title": "Agriculture news"}, "entries": [{"title": "Study identifies key agricultural practices that threaten soil health and global food supply", "summary": "The global food system faces growing risks as modern farming practices undermine the resilience of the world's soils, according to new research.", "link": "https://phys.org/news/2025-10-key-agricultural-threaten-soil-health.html", "published_js": "2025-10-01", "author": "None"}, {"title": "Global analysis assesses livestock vulnerability to climate change", "summary": "With a pioneering, comprehensive approach on a global scale, Brazilian researchers have developed a methodology that allows them to project the physiological responses of herds of different production animal species to the impacts of climate change between 2050 and 2100.", "link": "https://phys.org/news/2025-10-global-analysis-livestock-vulnerability-climate.html", "published_js": "2025-10-01", "author": "None"}, {"title": "'Every seed counts': Study compares drying conditions for seed rice performance", "summary": "Rice grown for seed rice was the focus of a yearlong investigation by the Arkansas Rice Processing Program using X\u2010ray imaging to determine the impact of higher drying temperatures.", "link": "https://phys.org/news/2025-10-seed-drying-conditions-rice.html", "published_js": "2025-10-01", "author": "None"}, {"title": "From space science to dinner plates: Reimagining the future of farming indoors", "summary": "Extreme weather events, from heavy rainfall to heat waves and droughts, are increasingly threatening crop yields globally, so new solutions are needed for agriculture.", "link": "https://phys.org/news/2025-10-space-science-dinner-plates-reimagining.html", "published_js": "2025-10-01", "author": "None"}, {"title": "Farming's environmental footprint shrinks, but progress has been uneven across England", "summary": "England's farms have significantly reduced their environmental footprint over the past decade, according to new modeling that suggests greenhouse gas emissions and other forms of pollution are on a downward trend.", "link": "https://phys.org/news/2025-09-farming-environmental-footprint-uneven-england.html", "published_js": "2025-10-01", "author": "None"}, {"title": "Bacteria could help fix the smoky taste of wildfire-tainted wine", "summary": "New laboratory experiments show that a bacterium that lives on grape plants can break down guaiacol\u2014an unpleasant-tasting substance which ruins wines made from grapes exposed to wildfire smoke. Claudia Castro of the U.S. Department of Agriculture's Agricultural Research Service and colleagues present these findings in...", "link": "https://phys.org/news/2025-09-bacteria-smoky-wildfire-tainted-wine.html", "published_js": "2025-10-01", "author": "None"}, {"title": "Cereal plants absorb nanoplastics, initial lab trials suggest", "summary": "Microplastics and nanoplastics in soils are a growing environmental problem. The extent to which agricultural crops absorb these particles and whether they end up in food has so far been difficult to prove. This is because they are hard to distinguish from plant components and...", "link": "https://phys.org/news/2025-10-cereal-absorb-nanoplastics-lab-trials.html", "published_js": "2025-10-01", "author": "None"}]}}, "Nature": {"http://feeds.feedburner.com/DiscoverLivingWorld": {"feed": {"title": "Planet Earth | Discover Magazine"}, "entries": []}, "http://feeds.feedburner.com/DiscoverEnvironment": {"feed": {"title": "Environment | Discover Magazine"}, "entries": []}}, "Business": {}, "Foss_Self-hosting": {}, "History": {}, "News": {"https://www.livemint.com/rss/politics/": {"feed": {"title": "mint - politics"}, "entries": [{"title": "\u2018Democracy under attack\u2019, says Rahul Gandhi in Colombia; BJP quips \u2018he loves China\u2019, \u2018insults India\u2019", "summary": "Congress leader Rahul Gandhi said at an event at EIA University of Colombia on Thursday (October 2) he is \u201cvery optimistic about India, but at the same time, there are fault lines within India\u2019s system that it has to overcome.\u201d", "link": "https://www.livemint.com/politics/news/democracy-under-attack-says-rahul-gandhi-in-colombia-bjp-quips-he-loves-china-insults-india-11759408472133.html", "published_js": "2025-10-02", "author": "None"}, {"title": "Supreme Court Refuses to Let Trump Oust Fed\u2019s Cook for Now", "summary": "The US Supreme Court refused to allow President Donald Trump to immediately oust Federal Reserve Governor Lisa Cook while she sues to keep her job, dealing a setback to his efforts to exert more control over the central bank.", "link": "https://www.livemint.com/politics/news/supreme-court-refuses-to-let-trump-oust-fed-s-cook-for-now-11759334634688.html", "published_js": "2025-10-01", "author": "None"}]}, "https://www.livemint.com/rss/industry": {"feed": {"title": "mint - industry"}, "entries": [{"title": "After IT firms, US lawmakers put the H-1B screws on academia", "summary": "Under the current rules, universities, research institutions and non-profits can hire an unlimited number of foreigners through the H-1B programme. That could be about to change.", "link": "https://www.livemint.com/industry/h1b-visa-for-academics-visa-cap-enforcement-act-tom-cotton-h1b-bill-us-university-h1b-cap-11759387469682.html", "published_js": "2025-10-02", "author": "None"}, {"title": "Mint Explainer: Are zero-balance accounts about to become the new normal in banking?", "summary": "Draft norms seek to expand BSBD services with mobile banking, cheque books and free digital transactions", "link": "https://www.livemint.com/industry/banking/rbi-zero-balance-accounts-unlimited-digital-transactions-free-cheque-books-banking-11759389026192.html", "published_js": "2025-10-02", "author": "None"}, {"title": "Luxury soars, budget struggles: India's hotel boom isn't as it seems", "summary": "Luxury hotel rates in India have surged, while budget options remain affordable. Recent GST cuts for mid-market hotels could stimulate growth. However, the rapid expansion of hotel supply may lead to challenges in balancing demand, particularly in smaller markets.", "link": "https://www.livemint.com/industry/india-hotel-industry-growth-luxury-budget-disparity-gst-impact-11759376906744.html", "published_js": "2025-10-02", "author": "None"}, {"title": "Bank holiday today: Are banks closed today on October 2 for Gandhi Jayanti, Dussehra and Vijaya Dashami?", "summary": "Bank holiday today, 2 October: There is a bank holiday today at all banks in India, including SBI (State Bank of India), ICICI Bank, HDFC Bank, Punjab National Bank and others for Gandhi Jayanti, Dussehra and Vijaya Dashami", "link": "https://www.livemint.com/industry/banking/bank-holiday-today-are-banks-closed-today-on-october-2-for-gandhi-jayanti-dussehra-and-vijaya-dashami-11759366522399.html", "published_js": "2025-10-02", "author": "None"}, {"title": "RBI to ease norms for banks\u2019 corporate exposure via market securities, to repeal earlier framework", "summary": "RBI governor Sanjay Malhotra said that the central bank is not worried about a resurgence in concentration risk given the introduction of other frameworks monitoring corporate exposure since then, and given the significant reduction in corporate credit exposure of banks over this period.", "link": "https://www.livemint.com/industry/banking/rbi-to-ease-norms-for-banks-corporate-exposure-via-market-securities-to-repeal-earlier-framework-11759328317532.html", "published_js": "2025-10-01", "author": "None"}]}, "swarajyamag.com": {"feed": {"title": "swarajyamag.com"}, "entries": [{"title": "No Substitute To Swadeshi And Self-Reliance: Key Takeaways From RSS Chief Mohan Bhagwat's Speech On Sangh's Centenary", "summary": "<a href=\" target=\"_blank\">No Substitute To Swadeshi And Self-Reliance: Key Takeaways From RSS Chief Mohan Bhagwat's Speech On Sangh's Centenary</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">", "link": "https://news.google.com/rss/articles/CBMi8AFBVV95cUxOTXZ4TTkwd01GZUJSaDRpR1JyX0FySGVfUlE5aTh4NENuUzlIaGhOVHZTOXFaQTJfVXdaS1NvcEg2aU1OaGNoQ1NZWUhGalZfMDdnWC1xOTBRamdDMHZhUl9zeldBd00wbUNxUFB1Ujl2ZEtmTVY5QlpqY3VNaHpianBEOXl4YmVsbC1DUFhzMTNvUGEyc0FXXzBOSG03TldOcmJ1R3pIN2tqSXZoclh3eGZhMlQ2ZUs3Uk1uZ1NWVjg1OGJsWnpvMGdaMHpHRDJLcENaWXdoNk5ub3BWb2tsS2tGM2VxRHBHUVVDcHVTdnDSAfABQVVfeXFMTk12eE05MHdNRmVCUmg0aUdScl9BckhlX1JROWk4eDRDblM5SGhoTlR2UzlxWkEyX1V3WktTb3BINmlNTmhjaENTWVlIRmpWXzA3Z1gtcTkwUWpnQzB2YVJfc3pXQXdNMG1DcVBQdVI5dmRLZk1WOUJaamN1TWh6YmpwRDl5eGJlbGwtQ1BYczEzb1BhMnNBV18wTkhtN05XTnJidUd6SDdrakl2aHJYd3hmYTJUNmVLN1JNbmdTVlY4NThibFp6bzBnWjB6R0QyS3BDWll3aDZObm9wVm9rbEtrRjNlcURwR1FVQ3B1U3Zw?oc=5", "published_js": "2025-10-02", "author": "None"}, {"title": "What Latest NCRB Data Says About Law And Order In Different States; Centre's Advance Tax Devolution For Festive Season; And More", "summary": "<a href=\" target=\"_blank\">What Latest NCRB Data Says About Law And Order In Different States; Centre's Advance Tax Devolution For Festive Season; And More</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">", "link": "https://news.google.com/rss/articles/CBMiggJBVV95cUxOLWRJekNYRk4tbmphV1BOY0hZT0VSd1otVGFtMm9sMlpsVW5TblNHTlJ4aUp2SGFWUHY4dmU3aE1URVNoUXdTLWkwSmZyZjVaU19MblNnSDdPTGh0Z005SWtBQ09iUzZNaDB1Z0VsMUJoTnpibmxLNlFqeTUweFlWeWNXUDdFYkFhRzNlakEtOUhLNURDQk9HamZwNlIyY19URmFSYWt1RzRQR3ByVkFWTVRwdU94R05Xa1piWjZ0Mi1fQVdqc0ZxTkxmdU1TbE52NzJtaXRwZTZWV08yZDRTWm8xVWw0NExHNWhyWTlTb3dRQ0hGSlBtaFpkMDZGQmlid1HSAYICQVVfeXFMTi1kSXpDWEZOLW5qYVdQTmNIWU9FUndaLVRhbTJvbDJabFVuU25TR05SeGlKdkhhVlB2OHZlN2hNVEVTaFF3Uy1pMEpmcmY1WlNfTG5TZ0g3T0xodGdNOUlrQUNPYlM2TWgwdWdFbDFCaE56Ym5sSzZRank1MHhZVnljV1A3RWJBYUczZWpBLTlISzVEQ0JPR2pmcDZSMmNfVEZhUmFrdUc0UEdwclZBVk1UcHVPeEdOV2taYlo2dDItX0FXanNGcU5MZnVNU2xOdjcybWl0cGU2VldPMmQ0U1pvMVVsNDRMRzVoclk5U293UUNIRkpQbWhaZDA2RkJpYndR?oc=5", "published_js": "2025-10-02", "author": "None"}]}}, "Reddit": {"https://oauth.reddit.com/r/todayilearned/top": {"feed": {"title": "Reddit - TIL"}, "entries": [{"title": "TIL the Jane Goodall Institute complained about one of Gary Larson's cartoons of her. She told them to be quiet, used the image to sell tshirts, and wrote the introduction to one of his collections", "summary": "", "link": "https://screenrant.com/far-side-controversial-comic-strip-jane-goodall/", "author": "None"}, {"title": "TIL that the 90s-early 2000s icon Eliza Dushku was \"inundated\" with fan mails from prisoners due to her portrayal of Faith in the show Buffy The Vampire Slayer", "summary": "", "link": "https://en.wikipedia.org/wiki/Eliza_Dushku#1992%E2%80%931997:_Beginnings_and_breakthrough", "author": "None"}, {"title": "TIL that in 1999, a 15-year-old named Jonathan James hacked into NASA\u2019s computers, accessed source code used for the International Space Station, and forced NASA to shut down parts of its systems for 21 days", "summary": "", "link": "https://www.justice.gov/archive/opa/pr/2000/September/555crm.htm", "author": "None"}, {"title": "TIL Hedgehogs can suffer from balloon syndrome, a rare condition where an infection to the skin causes it to inflate", "summary": "", "link": "https://en.wikipedia.org/wiki/Balloon_syndrome?repost", "author": "None"}, {"title": "TIL during the Victorian Era in London, people were scavenging for the fecal matter of dogs. This resource was valuable for leather tanning. The people were called \"pure finders.\"", "summary": "", "link": "https://en.wikipedia.org/wiki/London_Labour_and_the_London_Poor", "author": "None"}, {"title": "TIL the asteroid that killed the dinosaurs (~10 km wide) was no where near the biggest to ever hit Earth \u2014 earlier impacts were caused by asteroids 20\u201325 km across, like the one that made the Vredefort crater", "summary": "", "link": "https://en.wikipedia.org/wiki/Vredefort_impact_structure", "author": "None"}, {"title": "TIL that Oymyakon, a remote village in Siberia, holds the record for the coldest temperature ever recorded in an inhabited place: \u221267.7\u00b0C (\u221289.9\u00b0F) on February 6, 1933 . Despite its name meaning \u201cwater that doesn\u2019t freeze,\u201d everything in Oymyakon freezes.", "summary": "", "link": "https://guinnessworldrecords.com/world-records/lowest-temperature-inhabited", "author": "None"}, {"title": "TIL Th\u00e9r\u00e8se of Lisieux, a saint in the catholic church, was once dragged away from Pope Leo XIII while she was petitioning him during a pilgrimage.", "summary": "", "link": "https://en.wikipedia.org/wiki/Th%C3%A9r%C3%A8se_of_Lisieux#Rome_and_entry_to_Carmel", "author": "None"}, {"title": "TIL the chequered pattern associated with emergency services, mostly policing, is called the 'Sillitoe tartan' after Percy Sillitoe, Chief Constable of the Glasgow Police.", "summary": "", "link": "https://en.wikipedia.org/wiki/Sillitoe_tartan?wprov=sfla1", "author": "None"}, {"title": "TIL that there's a lake in Canada officially named \"Big Ass Lake\", named in 1953.", "summary": "", "link": "https://geonames.nrcan.gc.ca/search-place-names/unique?id=CACXC", "author": "None"}, {"title": "TIL the National Anthem for the People\u2019s Republic of China was originally written for the movie \u201c Children of Troubled Times\u201d (1935), and didn\u2019t become official until 2004", "summary": "", "link": "https://en.wikipedia.org/wiki/Children_of_Troubled_Times", "author": "None"}, {"title": "TIL The Guinness World Record for the largest feet on a living person is held by\u00a0Jeison Orlando Rodr\u00edguez Hern\u00e1ndez\u00a0from Venezuela.\u00a0his feet measured 40.57 cm (1 ft 3.96 in).", "summary": "", "link": "https://www.guinnessworldrecords.com/world-records/largest-feet-on-a-living-person-male", "author": "None"}]}, "https://oauth.reddit.com/.json": {"feed": {"title": "Reddit - TIL"}, "entries": [{"title": "OpenWebUI and the OpenAI compatible API", "summary": "Is there somewhere an option to save chats that are conducted via the API-endpoint (e.g. via  like if they are done via the browser chat-page?\n\nThat would be great to figure out what certain apps are prompting etc. and have it in some nice readable format.", "link": "https://www.reddit.com/r/OpenWebUI/comments/1nvxn5i/openwebui_and_the_openai_compatible_api/", "author": "None"}, {"title": "Dynamic GLM-4.6 Unsloth GGUFs out now!", "summary": "All the sizes have now been uploaded! Includes our chat template fixes too. You need the latest llama.cpp!\n\nWe had to fix multiple chat template issues for GLM 4.6 to make llama.cpp/llama-cli --jinja work - please only use --jinja otherwise the output will be wrong!\n\nSmallest 1-bit is 84.1 GiB, 4-bit is 204GiB. Remember they're GiB which is slightly larger than that Gigabytes GB so technically 84.1 GiB = 78.3 GB. Very confusing I know.\n\nLet us know how they are and we're excited for Air if it comes! :)", "link": "https://huggingface.co/unsloth/GLM-4.6-GGUF", "author": "None"}, {"title": "Data Fusion is Here: Biometric indexing is mapping separate text corpora to a single user identity.", "summary": "I usually focus on NLP models, but a simple test on the visual front showed me something terrifying about how cross-domain data is being unified.\n\nI ran a quick audit, starting with faceseek, just to see if it could locate my old identity. The shock wasn't that it found my old photo, but that it used that photo to link three completely different text-based corpora I manage: a highly professional technical blog, a casual Reddit account, and an anonymous political forum account.\n\nThese text personas had zero linguistic overlap or direct digital connection. This suggests the image-to-text-to-image pipeline is robust enough to use the biometric key as the fundamental unifying element. For those of us training large language models: Are we failing to protect the pseudonymity of our users because our training data is being silently cross-indexed by visual models? This fundamentally changes how we view data segmentation.", "link": "https://www.reddit.com/r/LanguageTechnology/comments/1nvcq19/data_fusion_is_here_biometric_indexing_is_mapping/", "author": "None"}, {"title": "The Forgotten Nazi Death Trap at Ozarichi", "summary": "", "link": "https://peakd.com/history/@arraymedia/the-forgotten-nazi-death-trap-at-ozarichi", "author": "None"}, {"title": "Canadian Foreign Minister Anita Anand to visit India on October 13-14, meets External Affairs Minister Jaishankar in New York", "summary": "", "link": "https://www.thehindu.com/news/national/india-canada-ties-jaishankar-meets-anita-anand-delhi-visit-nijjar-killing-pannun-plot/article70112847.ece", "author": "None"}, {"title": "China launches a new visa to attract tech talent, but locals aren't happy", "summary": "", "link": "https://www.bbc.com/news/articles/cvg4eeerzrwo", "author": "None"}, {"title": "Open-source lightweight, fast, expressive Kani TTS model", "summary": "Hi everyone!\n\nThanks for the awesome feedback on our first KaniTTS release!\n\nWe\u2019ve been hard at work, and released\u00a0[kani-tts-370m](\n\nIt\u2019s still built for speed and quality on consumer hardware, but now with expanded language support and more English voice options.\n\n# What\u2019s New:\n\n* **Multilingual Support**: German, Korean, Chinese, Arabic, and Spanish (with fine-tuning support). Prosody and naturalness improved across these languages.\n* **More English Voices**: Added a variety of new English voices.\n* **Architecture**: Same two-stage pipeline (LiquidAI LFM2-370M backbone + NVIDIA NanoCodec). Trained on \\~80k hours of diverse data.\n* **Performance**: Generates 15s of audio in \\~0.9s on an RTX 5080, using 2GB VRAM.\n* **Use Cases**: Conversational AI, edge devices, accessibility, or research.\n\nIt\u2019s still Apache 2.0 licensed, so dive in and experiment.\n\n**Repo**:\u00a0[  \n**Model**:\u00a0[\u00a0**Space**:\u00a0[  \n**Website**:\u00a0[\n\nLet us know what you think, and share your setups or use cases", "link": "https://huggingface.co/nineninesix/kani-tts-370m", "author": "None"}, {"title": "TypingSVG 5.0 \u2014 Support for non-monospace fonts with customizable uppercase/lowercase spacing", "summary": "", "link": "https://github.com/whiteSHADOW1234/TypingSVG", "author": "None"}, {"title": "Flowise API \"no connection adapters found\"", "summary": "Correct Flowise API and URL, yet it says \"No connection adapters were found for...\" \nI have absolutely no idea on how to fix this. Any help would be appreciated. ", "link": "https://www.reddit.com/r/OpenWebUI/comments/1nw060y/flowise_api_no_connection_adapters_found/", "author": "None"}, {"title": "Indian peacekeeping forces will not be deployed in Ukraine or Gaza outside of UN mandate", "summary": "", "link": "https://theprint.in/diplomacy/indian-peacekeeping-forces-will-not-be-deployed-in-ukraine-or-gaza-outside-of-un-mandate/2755330/", "author": "None"}, {"title": "Jane Goodall, famed primatologist, anthropologist, dead at 91", "summary": "", "link": "https://abcnews.go.com/amp/International/jane-goodall-famed-primatologist-anthropologist-conservationist-dead-91/story?id=109868347", "author": "None"}, {"title": "EAM\u2019s statement at the General Debate of the 80th session of the UNGA (September 27, 2025)", "summary": "", "link": "https://www.mea.gov.in/Speeches-Statements.htm?dtl/40170/EAMs+statement+at+the+General+Debate+of+the+80th+session+of+the+UNGA+September+27+2025", "author": "None"}, {"title": "Finepoint | Russia Is Key Partner In Modi\u2019s Calculus: India\u2019s Arctic Entry, Russia\u2019s Nuclear Edge", "summary": "", "link": "https://www.news18.com/opinion/finepoint-russia-is-key-partner-in-modis-calculus-indias-arctic-entry-russias-nuclear-edge-ws-kl-9607966.html", "author": "None"}, {"title": "India-European Free Trade Association (EFTA) free trade deal comes into force; to strengthen \u2018Make in India\u2019: Goyal", "summary": "", "link": "https://v.redd.it/vc4g3bksdpsf1", "author": "None"}, {"title": "Radio Unheard from r/unheard", "summary": "", "link": "https://martinkondor.github.io/RadioUnheard/", "author": "None"}, {"title": "Well I will be the real Jason vorhees .. I will be the best serial killer in history", "summary": "Real", "link": "https://www.reddit.com/r/speechtech/comments/1nvy8ji/well_i_will_be_the_real_jason_vorhees_i_will_be/", "author": "None"}, {"title": "Solar leads EU electricity generation as renewables hit 54%", "summary": "", "link": "https://electrek.co/2025/09/30/solar-leads-eu-electricity-generation-as-renewables-hit-54-percent/", "author": "None"}, {"title": "Canada and India must put economics at the heart of rapprochement", "summary": "", "link": "https://www.theglobeandmail.com/business/commentary/article-canada-india-economics-relations-reset/", "author": "None"}, {"title": "Four Caribbean nations sign deal allowing citizens to move freely without visas or work permits", "summary": "", "link": "https://abcnews.go.com/International/wireStory/caribbean-nations-sign-deal-allowing-citizens-move-freely-126123613", "author": "None"}, {"title": "IBM Granite 4.0 - Unsloth GGUFs &amp; Fine-tuning out now!", "summary": "IBM releases Granite-4.0, their new series of models! Run the 7B model on just 8GB RAM or 32B MoE on 40GB RAM.with Unsloth Dynamic GGUFs or fine-tune via our free notebook!\n\n* **Granite-4.0-H-Small (MoE):** Enterprise workhorse for daily tasks, supports multiple long-context sessions on entry GPUs like L40S (32B total, 9B active).\n* **Granite-4.0-H-Tiny (MoE):** Fast, cost-efficient for high-volume, low-complexity tasks; optimized for local and edge use (7B total, 1B active).\n* **Granite-4.0-H-Micro (Dense):** Lightweight, efficient for high-volume, low-complexity workloads; ideal for local and edge deployment (3B total).\n* **Micro (Dense):** Alternative dense option when Mamba2 isn\u2019t fully supported (3B total).\n\nAll model uploads: [\n\nGuide: [", "link": "https://i.redd.it/rofdtn5bepsf1.png", "author": "None"}, {"title": "RuntimeError for example notebook Gemma3_(4B)-Vision on Databricks", "summary": "I'm running into a RuntimeError while testing the Gemma3\\_(4B)-Vision.ipynb example notebook on Databricks, and was hoping for some guidance.\n\n**The problem:**\n\nThe notebook runs successfully up until the training step (trainer.train()), where it fails with a RuntimeError, I posted the output at the bottom of this post.  \n**This trainer only fails when unsloth's compilation is enabled**, training works correctly when I disable it through:\n\nos.environ\\[\"UNSLOTH\\_COMPILE\\_DISABLE\"\\] = \"1\"\n\nI'm running the example code without any modifications. The data loading and model setup appear to complete without any issues.\n\n**Environment details:**\n\nPlatform: Databricks Runtime 16.4 ML\n\nGPU: NVIDIA A10\n\nInstallation Method: I installed unsloth from GitHub using this command:\n\npip install \"unsloth\\[cu124-ampere-torch260\\] @ git+\n\nHas anyone seen this error before, particularly on Databricks? Any suggestions on what to investigate would be greatly appreciated.\n\nThanks in advance for your help! \ud83d\ude4f\n\n\n\n\\---------------------------------------------------------------\n\n\n\nRuntimeError: !dynamicLayerStack.empty() INTERNAL ASSERT FAILED at \"/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp\":219, please report a bug to PyTorch.File /databricks/python/lib/python3.12/site-packages/torch/\\_dynamo/eval\\_frame.py:574, in \\_TorchDynamoContext.\\_\\_call\\_\\_.&lt;locals&gt;.\\_fn(\\*args, \\*\\*kwargs)573 try:\n\n\\--&gt; 574return fn(\\*args, \\*\\*kwargs)575 finally:576# Restore the dynamic layer stack depth if necessary.\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_dynamo/eval\\_frame.py:574, in \\_TorchDynamoContext.\\_\\_call\\_\\_.&lt;locals&gt;.\\_fn(\\*args, \\*\\*kwargs)573 try:\n\n\\--&gt; 574return fn(\\*args, \\*\\*kwargs)575 finally:576# Restore the dynamic layer stack depth if necessary.\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_dynamo/convert\\_frame.py:1380, in CatchErrorsWrapper.\\_\\_call\\_\\_(self, frame, cache\\_entry, frame\\_state)1378 with compile\\_lock, \\_disable\\_current\\_modes():1379# skip=1: skip this frame\n\n\\-&gt; 1380return self.\\_torchdynamo\\_orig\\_callable(1381frame, cache\\_entry, self.hooks, frame\\_state, skip=11382)\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_dynamo/convert\\_frame.py:547, in ConvertFrameAssert.\\_\\_call\\_\\_(self, frame, cache\\_entry, hooks, frame\\_state, skip)546 with compile\\_context(CompileContext(compile\\_id)):\n\n\\--&gt; 547return \\_compile(548frame.f\\_code,549frame.f\\_globals,550frame.f\\_locals,551frame.f\\_builtins,552frame.closure,553self.\\_torchdynamo\\_orig\\_callable,554self.\\_one\\_graph,555self.\\_export,556self.\\_export\\_constraints,557hooks,558cache\\_entry,559cache\\_size,560frame,561frame\\_state=frame\\_state,562compile\\_id=compile\\_id,563skip=skip + 1,564)\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_dynamo/convert\\_frame.py:986, in \\_compile(code, globals, locals, builtins, closure, compiler\\_fn, one\\_graph, export, export\\_constraints, hooks, cache\\_entry, cache\\_size, frame, frame\\_state, compile\\_id, skip)985 try:\n\n\\--&gt; 986guarded\\_code = compile\\_inner(code, one\\_graph, hooks, transform)988# NB: We only put\\_code\\_state in success case.Success case here989# does include graph breaks; specifically, if a graph break still990# resulted in a partially compiled graph, we WILL return here.An(...)995# to upload for graph break though, because this can prevent996# extra graph break compilations.)\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_dynamo/convert\\_frame.py:715, in \\_compile.&lt;locals&gt;.compile\\_inner(code, one\\_graph, hooks, transform)714stack.enter\\_context(CompileTimeInstructionCounter.record())\n\n\\--&gt; 715return \\_compile\\_inner(code, one\\_graph, hooks, transform)717 return None\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_utils\\_internal.py:95, in compile\\_time\\_strobelight\\_meta.&lt;locals&gt;.compile\\_time\\_strobelight\\_meta\\_inner.&lt;locals&gt;.wrapper\\_function(\\*args, \\*\\*kwargs)94 if not StrobelightCompileTimeProfiler.enabled:\n\n\\---&gt; 95return function(\\*args, \\*\\*kwargs)97 return StrobelightCompileTimeProfiler.profile\\_compile\\_time(98function, phase\\_name, \\*args, \\*\\*kwargs99 )\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_dynamo/convert\\_frame.py:750, in \\_compile.&lt;locals&gt;.\\_compile\\_inner(code, one\\_graph, hooks, transform)749 try:\n\n\\--&gt; 750out\\_code = transform\\_code\\_object(code, transform)751break\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_dynamo/bytecode\\_transformation.py:1361, in transform\\_code\\_object(code, transformations, safe)1359 propagate\\_line\\_nums(instructions)\n\n\\-&gt; 1361 transformations(instructions, code\\_options)1362 return clean\\_and\\_assemble\\_instructions(instructions, keys, code\\_options)\\[1\\]\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_dynamo/convert\\_frame.py:231, in preserve\\_global\\_state.&lt;locals&gt;.\\_fn(\\*args, \\*\\*kwargs)230 try:\n\n\\--&gt; 231return fn(\\*args, \\*\\*kwargs)232 finally:\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_dynamo/convert\\_frame.py:662, in \\_compile.&lt;locals&gt;.transform(instructions, code\\_options)661with tracing(tracer.output.tracing\\_context), tracer.set\\_current\\_tx():\n\n\\--&gt; 662tracer.run()663 except exc.UnspecializeRestartAnalysis:\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_dynamo/symbolic\\_convert.py:2868, in InstructionTranslator.run(self)2867 def run(self):\n\n\\-&gt; 2868super().run()\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_dynamo/symbolic\\_convert.py:1052, in InstructionTranslatorBase.run(self)1051 self.output.push\\_tx(self)\n\n\\-&gt; 1052 while self.step():1053pass\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_dynamo/symbolic\\_convert.py:962, in InstructionTranslatorBase.step(self)961 try:\n\n\\--&gt; 962self.dispatch\\_table\\[inst.opcode\\](self, inst)963return not self.output.should\\_exit\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_dynamo/symbolic\\_convert.py:3051, in InstructionTranslator.RETURN\\_CONST(self, inst)3050 def RETURN\\_CONST(self, inst):\n\n\\-&gt; 3051self.\\_return(inst)\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_dynamo/symbolic\\_convert.py:3033, in InstructionTranslator.\\_return(self, inst)3032 log.debug(\"%s triggered compile\", inst.opname)\n\n\\-&gt; 3033 self.output.compile\\_subgraph(3034self,3035reason=GraphCompileReason(3036\"return\\_value\", \\[self.frame\\_summary()\\], graph\\_break=False3037),3038 )3039 return\\_inst = (3040create\\_instruction(\"RETURN\\_VALUE\")3041if inst.opname == \"RETURN\\_VALUE\"3042else create\\_instruction(\"RETURN\\_CONST\", argval=inst.argval)3043 )\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_dynamo/output\\_graph.py:1136, in OutputGraph.compile\\_subgraph(self, tx, partial\\_convert, reason)1134 if count\\_calls(self.graph) != 0 or len(pass2.graph\\_outputs) != 0:1135output.extend(\n\n\\-&gt; 1136self.compile\\_and\\_call\\_fx\\_graph(1137tx, pass2.graph\\_output\\_vars(), root, output\\_replacements1138)1139)1141if len(pass2.graph\\_outputs) != 0:\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_dynamo/output\\_graph.py:1382, in OutputGraph.compile\\_and\\_call\\_fx\\_graph(self, tx, rv, root, replaced\\_outputs)1381 with self.restore\\_global\\_state():\n\n\\-&gt; 1382compiled\\_fn = self.call\\_user\\_compiler(gm)1384 from torch.fx.\\_lazy\\_graph\\_module import \\_LazyGraphModule\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_dynamo/output\\_graph.py:1432, in OutputGraph.call\\_user\\_compiler(self, gm)1426 with dynamo\\_timed(1427\"OutputGraph.call\\_user\\_compiler\",1428phase\\_name=\"backend\\_compile\",1429log\\_pt2\\_compile\\_event=True,1430dynamo\\_compile\\_column\\_us=\"aot\\_autograd\\_cumulative\\_compile\\_time\\_us\",1431 ):\n\n\\-&gt; 1432return self.\\_call\\_user\\_compiler(gm)\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_dynamo/output\\_graph.py:1483, in OutputGraph.\\_call\\_user\\_compiler(self, gm)1482 except Exception as e:\n\n\\-&gt; 1483raise BackendCompilerFailed(self.compiler\\_fn, e).with\\_traceback(1484e.\\_\\_traceback\\_\\_1485) from None1487 signpost\\_event(1488\"dynamo\",1489\"OutputGraph.call\\_user\\_compiler\",(...)1495},1496 )\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_dynamo/output\\_graph.py:1462, in OutputGraph.\\_call\\_user\\_compiler(self, gm)1461compiler\\_fn = WrapperBackend(compiler\\_fn)\n\n\\-&gt; 1462 compiled\\_fn = compiler\\_fn(gm, self.example\\_inputs())1463 \\_step\\_logger()(logging.INFO, f\"done compiler function {name}\")\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_dynamo/repro/after\\_dynamo.py:130, in WrapBackendDebug.\\_\\_call\\_\\_(self, gm, example\\_inputs, \\*\\*kwargs)129 else:\n\n\\--&gt; 130compiled\\_gm = compiler\\_fn(gm, example\\_inputs)132 return compiled\\_gm\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_dynamo/repro/after\\_dynamo.py:130, in WrapBackendDebug.\\_\\_call\\_\\_(self, gm, example\\_inputs, \\*\\*kwargs)129 else:\n\n\\--&gt; 130compiled\\_gm = compiler\\_fn(gm, example\\_inputs)132 return compiled\\_gm\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_\\_init\\_\\_.py:2340, in \\_TorchCompileInductorWrapper.\\_\\_call\\_\\_(self, model\\_, inputs\\_)2338 from torch.\\_inductor.compile\\_fx import compile\\_fx\n\n\\-&gt; 2340 return compile\\_fx(model\\_, inputs\\_, config\\_patches=self.config)\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_inductor/compile\\_fx.py:1552, in compile\\_fx(model\\_, example\\_inputs\\_, inner\\_compile, config\\_patches, decompositions)1551with config.patch(config\\_patches):\n\n\\-&gt; 1552return compile\\_fx(1553model\\_,1554example\\_inputs\\_,1555# need extra layer of patching as backwards is compiled out of scope1556inner\\_compile=config.patch(config\\_patches)(inner\\_compile),1557decompositions=decompositions,1558)1560 # TODO: This probably shouldn't be a recursive call\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_inductor/compile\\_fx.py:1863, in compile\\_fx(model\\_, example\\_inputs\\_, inner\\_compile, config\\_patches, decompositions)1858 with V.set\\_fake\\_mode(fake\\_mode), torch.\\_guards.tracing(1859tracing\\_context1860 ), compiled\\_autograd.\\_disable(), functorch\\_config.patch(1861unlift\\_effect\\_tokens=True1862 ):\n\n\\-&gt; 1863return aot\\_autograd(1864fw\\_compiler=fw\\_compiler,1865bw\\_compiler=bw\\_compiler,1866inference\\_compiler=inference\\_compiler,1867decompositions=decompositions,1868partition\\_fn=partition\\_fn,1869keep\\_inference\\_input\\_mutations=True,1870cudagraphs=cudagraphs,1871)(model\\_, example\\_inputs\\_)\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_dynamo/backends/common.py:83, in AotAutograd.\\_\\_call\\_\\_(self, gm, example\\_inputs, \\*\\*kwargs)82 with enable\\_aot\\_logging(), patch\\_config:\n\n\\---&gt; 83cg = aot\\_module\\_simplified(gm, example\\_inputs, \\*\\*self.kwargs)84counters\\[\"aot\\_autograd\"\\]\\[\"ok\"\\] += 1\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_functorch/aot\\_autograd.py:1155, in aot\\_module\\_simplified(mod, args, fw\\_compiler, bw\\_compiler, partition\\_fn, decompositions, keep\\_inference\\_input\\_mutations, inference\\_compiler, cudagraphs)1154 else:\n\n\\-&gt; 1155compiled\\_fn = dispatch\\_and\\_compile()1157 if isinstance(mod, torch.\\_dynamo.utils.GmWrapper):1158# This function is called by the flatten\\_graph\\_inputs wrapper, which boxes1159# the inputs so that they can be freed before the end of this scope.1160# For overhead reasons, this is not the default wrapper, see comment:1161#  target=\"\\_blank\" rel=\"noopener noreferrer\"&gt;\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_functorch/aot\\_autograd.py:1131, in aot\\_module\\_simplified.&lt;locals&gt;.dispatch\\_and\\_compile()1130 with compiled\\_autograd.\\_disable():\n\n\\-&gt; 1131compiled\\_fn, \\_ = create\\_aot\\_dispatcher\\_function(1132functional\\_call,1133fake\\_flat\\_args,1134aot\\_config,1135fake\\_mode,1136shape\\_env,1137)1138 return compiled\\_fn\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_functorch/aot\\_autograd.py:580, in create\\_aot\\_dispatcher\\_function(flat\\_fn, fake\\_flat\\_args, aot\\_config, fake\\_mode, shape\\_env)579 with dynamo\\_timed(\"create\\_aot\\_dispatcher\\_function\", log\\_pt2\\_compile\\_event=True):\n\n\\--&gt; 580return \\_create\\_aot\\_dispatcher\\_function(581flat\\_fn, fake\\_flat\\_args, aot\\_config, fake\\_mode, shape\\_env582)\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_functorch/aot\\_autograd.py:830, in \\_create\\_aot\\_dispatcher\\_function(flat\\_fn, fake\\_flat\\_args, aot\\_config, fake\\_mode, shape\\_env)828 compiler\\_fn = choose\\_dispatcher(needs\\_autograd, aot\\_config)\n\n\\--&gt; 830 compiled\\_fn, fw\\_metadata = compiler\\_fn(831flat\\_fn,832\\_dup\\_fake\\_script\\_obj(fake\\_flat\\_args),833aot\\_config,834fw\\_metadata=fw\\_metadata,835 )836 return compiled\\_fn, fw\\_metadata\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_functorch/\\_aot\\_autograd/jit\\_compile\\_runtime\\_wrappers.py:153, in aot\\_dispatch\\_base(flat\\_fn, flat\\_args, aot\\_config, fw\\_metadata)149 flat\\_fn, flat\\_args, fw\\_metadata = pre\\_compile(150wrappers, flat\\_fn, flat\\_args, aot\\_config, fw\\_metadata=fw\\_metadata151 )\n\n\\--&gt; 153 fw\\_module, updated\\_flat\\_args, maybe\\_subclass\\_meta = aot\\_dispatch\\_base\\_graph(# type: ignore\\[misc\\]154flat\\_fn, flat\\_args, aot\\_config, fw\\_metadata=fw\\_metadata155 )156 # Save the forward\\_graph\\_str right after aot\\_dispatch\\_base\\_graph,157 # to save in the cache\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_functorch/\\_aot\\_autograd/dispatch\\_and\\_compile\\_graph.py:153, in aot\\_dispatch\\_base\\_graph(flat\\_fn, flat\\_args, aot\\_config, fw\\_metadata)149saved\\_updated\\_flat\\_args\\_subclasses\\_desugared = pytree.tree\\_map\\_only(150torch.Tensor, lambda t: t.detach(), updated\\_flat\\_args\\_subclasses\\_desugared151)\n\n\\--&gt; 153 fw\\_module = \\_create\\_graph(154fn\\_to\\_trace,155updated\\_flat\\_args\\_subclasses\\_desugared,156aot\\_config=aot\\_config,157 )159 if aot\\_config.is\\_export and mod\\_when\\_exporting\\_non\\_strict is not None:160# We update metadata to consider any assigned buffers as buffer mutations.\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_functorch/\\_aot\\_autograd/dispatch\\_and\\_compile\\_graph.py:55, in \\_create\\_graph(f, args, aot\\_config)49 with enable\\_python\\_dispatcher(), FunctionalTensorMode(50pre\\_dispatch=aot\\_config.pre\\_dispatch,51export=aot\\_config.is\\_export,52# Allow token discovery for joint fn tracing as tokens can be used in backward.53\\_allow\\_token\\_discovery=True,54 ):\n\n\\---&gt; 55fx\\_g = make\\_fx(56f,57decomposition\\_table=aot\\_config.decompositions,58record\\_module\\_stack=True,59pre\\_dispatch=aot\\_config.pre\\_dispatch,60)(\\*args)62 return fx\\_g\n\nFile /databricks/python/lib/python3.12/site-packages/torch/fx/experimental/proxy\\_tensor.py:2196, in make\\_fx.&lt;locals&gt;.wrapped(\\*args)2194 u/functools.wraps(f)2195 def wrapped(\\*args: object) -&gt; GraphModule:\n\n\\-&gt; 2196return make\\_fx\\_tracer.trace(f, \\*args)\n\nFile /databricks/python/lib/python3.12/site-packages/torch/fx/experimental/proxy\\_tensor.py:2134, in \\_MakefxTracer.trace(self, f, \\*args)2133 with self.\\_init\\_modes\\_from\\_inputs(f, args):\n\n\\-&gt; 2134return self.\\_trace\\_inner(f, \\*args)\n\nFile /databricks/python/lib/python3.12/site-packages/torch/fx/experimental/proxy\\_tensor.py:2105, in \\_MakefxTracer.\\_trace\\_inner(self, f, \\*args)2104 try:\n\n\\-&gt; 2105t = dispatch\\_trace(2106wrap\\_key(func, args, self.fx\\_tracer, self.pre\\_dispatch),2107tracer=self.fx\\_tracer,2108concrete\\_args=tuple(phs),2109)2110 except Exception:\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_compile.py:32, in \\_disable\\_dynamo.&lt;locals&gt;.inner(\\*args, \\*\\*kwargs)30fn.\\_\\_dynamo\\_disable = disable\\_fn\n\n\\---&gt; 32 return disable\\_fn(\\*args, \\*\\*kwargs)\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_dynamo/eval\\_frame.py:745, in DisableContext.\\_\\_call\\_\\_.&lt;locals&gt;.\\_fn(\\*args, \\*\\*kwargs)744 try:\n\n\\--&gt; 745return fn(\\*args, \\*\\*kwargs)746 finally:\n\nFile /databricks/python/lib/python3.12/site-packages/torch/fx/experimental/proxy\\_tensor.py:1138, in dispatch\\_trace(root, tracer, concrete\\_args)1132 u/torch.\\_disable\\_dynamo1133 def dispatch\\_trace(1134root: Union\\[Module, Callable\\],1135tracer: Tracer,1136concrete\\_args: Optional\\[Tuple\\[Any, ...\\]\\] = None,1137 ) -&gt; GraphModule:\n\n\\-&gt; 1138graph = tracer.trace(root, concrete\\_args)# type: ignore\\[arg-type\\]1140# NB: be careful not to DCE .item() calls\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_dynamo/eval\\_frame.py:745, in DisableContext.\\_\\_call\\_\\_.&lt;locals&gt;.\\_fn(\\*args, \\*\\*kwargs)744 try:\n\n\\--&gt; 745return fn(\\*args, \\*\\*kwargs)746 finally:\n\nFile /databricks/python/lib/python3.12/site-packages/torch/fx/\\_symbolic\\_trace.py:843, in Tracer.trace(self, root, concrete\\_args)837\\_autowrap\\_check(838patcher, module.\\_\\_dict\\_\\_, self.\\_autowrap\\_function\\_ids839)840self.create\\_node(841\"output\",842\"output\",\n\n\\--&gt; 843(self.create\\_arg(fn(\\*args)),),844{},845type\\_expr=fn.\\_\\_annotations\\_\\_.get(\"return\", None),846)848 self.submodule\\_paths = None\n\nFile /databricks/python/lib/python3.12/site-packages/torch/fx/experimental/proxy\\_tensor.py:1193, in wrap\\_key.&lt;locals&gt;.wrapped(\\*proxies, \\*\\*\\_unused)1191return get\\_proxy\\_slot(t, tracer, t, lambda x: x.proxy)\n\n\\-&gt; 1193 out = f(\\*tensors)# type:ignore\\[call-arg\\]1194 out = pytree.tree\\_map\\_only(Tensor, get\\_tensor\\_proxy\\_slot, out)\n\nFile &lt;string&gt;:1, in &lt;lambda&gt;(arg0, arg1, arg2, arg3, arg4, arg5, arg6)\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_functorch/\\_aot\\_autograd/traced\\_function\\_transforms.py:693, in handle\\_effect\\_tokens\\_fn.&lt;locals&gt;.inner\\_fn(\\*args)692# Run the joint\n\n\\--&gt; 693outs = fn(\\*args)695 # Return both the tokens and the outputs696 # See Note \\[Side-Effectful Tokens in AOTAutograd\\]\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_functorch/\\_aot\\_autograd/traced\\_function\\_transforms.py:413, in create\\_functionalized\\_fn.&lt;locals&gt;.\\_functionalized\\_f\\_helper(\\*args)412# Run the joint\n\n\\--&gt; 413f\\_outs = fn(\\*f\\_args)415 if trace\\_joint:416# We support a limited amount of mutation of graph inputs during the backward pass.417# (This is used e.g. by Float8, which needs to update buffers during the backward pass)(...)425#the bw by running our analysis first on the fw-only graph, and then on the joint graph. This would426#require an extra round of tracing though, so it's more efficient to do in-line here.\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_functorch/\\_aot\\_autograd/traced\\_function\\_transforms.py:78, in fn\\_input\\_mutations\\_to\\_outputs.&lt;locals&gt;.inner\\_fn(\\*args)76 u/wraps(fn)77 def inner\\_fn(\\*args):\n\n\\---&gt; 78outs = fn(\\*args)79assert len(meta.output\\_info) == len(outs)\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_functorch/\\_aot\\_autograd/traced\\_function\\_transforms.py:875, in create\\_functional\\_call.&lt;locals&gt;.functional\\_call(\\*args, \\*\\*kwargs)874detect\\_fake\\_mode().epoch += 1\n\n\\--&gt; 875out = PropagateUnbackedSymInts(mod).run(876\\*args\\[params\\_len:\\], \\*\\*kwargs877)878 else:\n\nFile /databricks/python/lib/python3.12/site-packages/torch/fx/interpreter.py:167, in Interpreter.run(self, initial\\_env, enable\\_io\\_processing, \\*args)166 try:\n\n\\--&gt; 167self.env\\[node\\] = self.run\\_node(node)168 except Exception as e:\n\nFile /databricks/python/lib/python3.12/site-packages/torch/fx/experimental/symbolic\\_shapes.py:6779, in PropagateUnbackedSymInts.run\\_node(self, n)6777 from torch.\\_guards import detect\\_fake\\_mode\n\n\\-&gt; 6779 result = super().run\\_node(n)6780 rebind\\_unbacked(detect\\_fake\\_mode().shape\\_env, n, result)\n\nFile /databricks/python/lib/python3.12/site-packages/torch/fx/interpreter.py:230, in Interpreter.run\\_node(self, n)229 assert isinstance(kwargs, dict)\n\n\\--&gt; 230 return getattr(self, n.op)(n.target, args, kwargs)\n\nFile /databricks/python/lib/python3.12/site-packages/torch/fx/interpreter.py:310, in Interpreter.call\\_function(self, target, args, kwargs)309 # Execute the function and return the result\n\n\\--&gt; 310 return target(\\*args, \\*\\*kwargs)\n\nFile /local\\_disk0/.ephemeral\\_nfs/envs/pythonEnv-571e3abe-4219-4b3f-a998-7d01f4feeaa0/lib/python3.12/site-packages/unsloth\\_zoo/patch\\_torch\\_functions.py:150, in cross\\_entropy(input, target, weight, size\\_average, ignore\\_index, reduce, reduction, label\\_smoothing)149 if has\\_torch\\_function\\_variadic(input, target, weight):\n\n\\--&gt; 150return handle\\_torch\\_function(151cross\\_entropy,152(input, target, weight),153input,154target,155weight=weight,156size\\_average=size\\_average,157ignore\\_index=ignore\\_index,158reduce=reduce,159reduction=reduction,160label\\_smoothing=label\\_smoothing,161).to(input.dtype)162 if size\\_average is not None or reduce is not None:\n\nFile /databricks/python/lib/python3.12/site-packages/torch/overrides.py:1720, in handle\\_torch\\_function(public\\_api, relevant\\_args, \\*args, \\*\\*kwargs)1719 with \\_pop\\_mode\\_temporarily() as mode:\n\n\\-&gt; 1720result = mode.\\_\\_torch\\_function\\_\\_(public\\_api, types, args, kwargs)1721 if result is not NotImplemented:\n\nFile /databricks/python/lib/python3.12/site-packages/torch/fx/experimental/proxy\\_tensor.py:1241, in TorchFunctionMetadataMode.\\_\\_torch\\_function\\_\\_(self, func, types, args, kwargs)1240 self.tracer.torch\\_fn\\_counts\\[func\\] = self.tracer.torch\\_fn\\_counts.get(func, 0) + 1\n\n\\-&gt; 1241 return func(\\*args, \\*\\*kwargs)\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_dynamo/eval\\_frame.py:544, in \\_TorchDynamoContext.\\_\\_call\\_\\_.&lt;locals&gt;.\\_fn(\\*args, \\*\\*kwargs)543 if config.error\\_on\\_nested\\_fx\\_trace:\n\n\\--&gt; 544raise RuntimeError(545\"Detected that you are using FX to symbolically trace \"546\"a dynamo-optimized function. This is not supported at the moment.\"547)548 else:\n\nBackendCompilerFailed: backend='inductor' raised:\n\nRuntimeError: Detected that you are using FX to symbolically trace a dynamo-optimized function. This is not supported at the moment.While executing %loss : \\[num\\_users=1\\] = call\\_function\\[target=unsloth\\_zoo.patch\\_torch\\_functions.cross\\_entropy\\](args = (), kwargs = {input: %contiguous, target: %contiguous\\_1, reduction: sum})\n\nOriginal traceback:File \"/local\\_disk0/.ephemeral\\_nfs/envs/pythonEnv-571e3abe-4219-4b3f-a998-7d01f4feeaa0/lib/python3.12/site-packages/unsloth\\_zoo/fused\\_losses/cross\\_entropy\\_loss.py\", line 274, in accumulate\\_chunk(chunk\\_loss, (unscaled\\_loss,)) = torch.func.grad\\_and\\_value(File \"/databricks/python/lib/python3.12/site-packages/torch/\\_functorch/apis.py\", line 442, in wrapperreturn eager\\_transforms.grad\\_and\\_value\\_impl(File \"/databricks/python/lib/python3.12/site-packages/torch/\\_functorch/vmap.py\", line 48, in fnreturn f(\\*args, \\*\\*kwargs)File \"/databricks/python/lib/python3.12/site-packages/torch/\\_functorch/eager\\_transforms.py\", line 1364, in grad\\_and\\_value\\_imploutput = func(\\*args, \\*\\*kwargs)File \"/local\\_disk0/.ephemeral\\_nfs/envs/pythonEnv-571e3abe-4219-4b3f-a998-7d01f4feeaa0/lib/python3.12/site-packages/unsloth\\_zoo/fused\\_losses/cross\\_entropy\\_loss.py\", line 98, in compute\\_fused\\_ce\\_lossloss = torch.nn.functional.cross\\_entropy(File \"/databricks/python/lib/python3.12/site-packages/torch/\\_dynamo/polyfills/\\_\\_init\\_\\_.py\", line 160, in getattr\\_and\\_tracereturn fn(\\*args\\[2:\\], \\*\\*kwargs)Set TORCH\\_LOGS=\"+dynamo\" and TORCHDYNAMO\\_VERBOSE=1 for more informationDuring handling of the above exception, another exception occurred:\n\nRuntimeErrorTraceback (most recent call last)\n\nFile &lt;command-4501055251330983&gt;, line 1\n\n\\----&gt; 1 trainer\\_stats = trainer.train()\n\nFile /Workspace/Users/\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588/DataScience/ds\\_iteratie\\_28/\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588/ml\\_model\\_dev/notebooks/multimodaal/eda/modular\\_DS\\_001/understanding\\_tests\\_001/unsloth\\_compiled\\_cache/UnslothSFTTrainer.py:53, in prepare\\_for\\_training\\_mode.&lt;locals&gt;.wrapper(self, \\*args, \\*\\*kwargs)51 if hasattr(self, 'model') and hasattr(self.model, \"for\\_training\"):52self.model.for\\_training()\n\n\\---&gt; 53 output = f(self, \\*args, \\*\\*kwargs)54 # Return inference mode55 if hasattr(self, 'model') and hasattr(self.model, \"for\\_inference\"):\n\nFile /databricks/python/lib/python3.12/site-packages/mlflow/utils/autologging\\_utils/safety.py:402, in safe\\_patch.&lt;locals&gt;.safe\\_patch\\_function(\\*args, \\*\\*kwargs)384 if (385active\\_session\\_failed386or autologging\\_is\\_disabled(autologging\\_integration)(...)396# warning behavior during original function execution, since autologging is being397# skipped398with NonMlflowWarningsBehaviorForCurrentThread(399disable\\_warnings=False,400reroute\\_warnings=False,401):\n\n\\--&gt; 402return original(\\*args, \\*\\*kwargs)404 # Whether or not the original / underlying function has been called during the405 # execution of patched code406 original\\_has\\_been\\_called = False\n\nFile /databricks/python\\_shell/lib/dbruntime/huggingface\\_patches/transformers.py:54, in \\_create\\_patch\\_function.&lt;locals&gt;.patched\\_fit\\_function(self, \\*args, \\*\\*kwargs)52 call\\_succeeded = False53 try:\n\n\\---&gt; 54model = original\\_method(self, \\*args, \\*\\*kwargs)55call\\_succeeded = True56return model\n\nFile /local\\_disk0/.ephemeral\\_nfs/envs/pythonEnv-571e3abe-4219-4b3f-a998-7d01f4feeaa0/lib/python3.12/site-packages/transformers/trainer.py:2328, in Trainer.train(self, resume\\_from\\_checkpoint, trial, ignore\\_keys\\_for\\_eval, \\*\\*kwargs)2326hf\\_hub\\_utils.enable\\_progress\\_bars()2327 else:\n\n\\-&gt; 2328return inner\\_training\\_loop(2329args=args,2330resume\\_from\\_checkpoint=resume\\_from\\_checkpoint,2331trial=trial,2332ignore\\_keys\\_for\\_eval=ignore\\_keys\\_for\\_eval,2333)\n\nFile &lt;string&gt;:323, in \\_fast\\_inner\\_training\\_loop(self, batch\\_size, args, resume\\_from\\_checkpoint, trial, ignore\\_keys\\_for\\_eval)\n\nFile /Workspace/Users/\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588/DataScience/ds\\_iteratie\\_28/\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588/ml\\_model\\_dev/notebooks/multimodaal/eda/modular\\_DS\\_001/understanding\\_tests\\_001/unsloth\\_compiled\\_cache/UnslothSFTTrainer.py:1040, in \\_UnslothSFTTrainer.training\\_step(self, \\*args, \\*\\*kwargs)1038 def training\\_step(self, \\*args, \\*\\*kwargs):1039with self.maybe\\_activation\\_offload\\_context:\n\n\\-&gt; 1040return super().training\\_step(\\*args, \\*\\*kwargs)\n\nFile &lt;string&gt;:40, in \\_unsloth\\_training\\_step(self, model, inputs, num\\_items\\_in\\_batch)\n\nFile /Workspace/Users/\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588/DataScience/ds\\_iteratie\\_28/\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588/ml\\_model\\_dev/notebooks/multimodaal/eda/modular\\_DS\\_001/understanding\\_tests\\_001/unsloth\\_compiled\\_cache/UnslothSFTTrainer.py:1029, in \\_UnslothSFTTrainer.compute\\_loss(self, model, inputs, return\\_outputs, num\\_items\\_in\\_batch)1028 def compute\\_loss(self, model, inputs, return\\_outputs = False, num\\_items\\_in\\_batch = None):\n\n\\-&gt; 1029outputs = super().compute\\_loss(1030model,1031inputs,1032return\\_outputs = return\\_outputs,1033num\\_items\\_in\\_batch = num\\_items\\_in\\_batch,1034)1035return outputs\n\nFile /local\\_disk0/.ephemeral\\_nfs/envs/pythonEnv-571e3abe-4219-4b3f-a998-7d01f4feeaa0/lib/python3.12/site-packages/unsloth/models/\\_utils.py:1321, in \\_unsloth\\_pre\\_compute\\_loss(self, model, inputs, \\*args, \\*\\*kwargs)1315logger.warning\\_once(1316f\"Unsloth: Not an error, but {name} does not accept \\num\\_items\\_in\\_batch\\.\\\\n\"\\\\1317\"Using gradient accumulation will be very slightly less accurate.\\\\n\"\\\\1318\"Read more on gradient accumulation issues here:  target=\"\\_blank\" rel=\"noopener noreferrer\"&gt; pass\n\n\\-&gt; 1321 outputs = self.\\_old\\_compute\\_loss(model, inputs, \\*args, \\*\\*kwargs)1322 return outputs\n\nFile /local\\_disk0/.ephemeral\\_nfs/envs/pythonEnv-571e3abe-4219-4b3f-a998-7d01f4feeaa0/lib/python3.12/site-packages/transformers/trainer.py:4099, in Trainer.compute\\_loss(self, model, inputs, return\\_outputs, num\\_items\\_in\\_batch)4097kwargs\\[\"num\\_items\\_in\\_batch\"\\] = num\\_items\\_in\\_batch4098inputs = {\\*\\*inputs, \\*\\*kwargs}\n\n\\-&gt; 4099 outputs = model(\\*\\*inputs)4100 # Save past state if it exists4101 # TODO: this needs to be fixed and made cleaner later.4102 if self.args.past\\_index &gt;= 0:\n\nFile /databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1739, in Module.\\_wrapped\\_call\\_impl(self, \\*args, \\*\\*kwargs)1737return self.\\_compiled\\_call\\_impl(\\*args, \\*\\*kwargs)# type: ignore\\[misc\\]1738 else:\n\n\\-&gt; 1739return self.\\_call\\_impl(\\*args, \\*\\*kwargs)\n\nFile /databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1750, in Module.\\_call\\_impl(self, \\*args, \\*\\*kwargs)1745 # If we don't have any hooks, we want to skip the rest of the logic in1746 # this function, and just call forward.1747 if not (self.\\_backward\\_hooks or self.\\_backward\\_pre\\_hooks or self.\\_forward\\_hooks or self.\\_forward\\_pre\\_hooks1748or \\_global\\_backward\\_pre\\_hooks or \\_global\\_backward\\_hooks1749or \\_global\\_forward\\_hooks or \\_global\\_forward\\_pre\\_hooks):\n\n\\-&gt; 1750return forward\\_call(\\*args, \\*\\*kwargs)1752 result = None1753 called\\_always\\_called\\_hooks = set()\n\nFile /databricks/python/lib/python3.12/site-packages/accelerate/utils/operations.py:819, in convert\\_outputs\\_to\\_fp32.&lt;locals&gt;.forward(\\*args, \\*\\*kwargs)818 def forward(\\*args, \\*\\*kwargs):\n\n\\--&gt; 819return model\\_forward(\\*args, \\*\\*kwargs)\n\nFile /databricks/python/lib/python3.12/site-packages/accelerate/utils/operations.py:807, in ConvertOutputsToFp32.\\_\\_call\\_\\_(self, \\*args, \\*\\*kwargs)806 def \\_\\_call\\_\\_(self, \\*args, \\*\\*kwargs):\n\n\\--&gt; 807return convert\\_to\\_fp32(self.model\\_forward(\\*args, \\*\\*kwargs))\n\nFile /databricks/python/lib/python3.12/site-packages/torch/amp/autocast\\_mode.py:44, in autocast\\_decorator.&lt;locals&gt;.decorate\\_autocast(\\*args, \\*\\*kwargs)41 u/functools.wraps(func)42 def decorate\\_autocast(\\*args, \\*\\*kwargs):43with autocast\\_instance:\n\n\\---&gt; 44return func(\\*args, \\*\\*kwargs)\n\nFile /local\\_disk0/.ephemeral\\_nfs/envs/pythonEnv-571e3abe-4219-4b3f-a998-7d01f4feeaa0/lib/python3.12/site-packages/peft/peft\\_model.py:1850, in PeftModelForCausalLM.forward(self, input\\_ids, attention\\_mask, inputs\\_embeds, labels, output\\_attentions, output\\_hidden\\_states, return\\_dict, task\\_ids, \\*\\*kwargs)1848with self.\\_enable\\_peft\\_forward\\_hooks(\\*\\*kwargs):1849kwargs = {k: v for k, v in kwargs.items() if k not in self.special\\_peft\\_forward\\_args}\n\n\\-&gt; 1850return self.base\\_model(1851input\\_ids=input\\_ids,1852attention\\_mask=attention\\_mask,1853inputs\\_embeds=inputs\\_embeds,1854labels=labels,1855output\\_attentions=output\\_attentions,1856output\\_hidden\\_states=output\\_hidden\\_states,1857return\\_dict=return\\_dict,1858\\*\\*kwargs,1859)1861 batch\\_size = \\_get\\_batch\\_size(input\\_ids, inputs\\_embeds)1862 if attention\\_mask is not None:1863# concat prompt attention mask\n\nFile /databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1739, in Module.\\_wrapped\\_call\\_impl(self, \\*args, \\*\\*kwargs)1737return self.\\_compiled\\_call\\_impl(\\*args, \\*\\*kwargs)# type: ignore\\[misc\\]1738 else:\n\n\\-&gt; 1739return self.\\_call\\_impl(\\*args, \\*\\*kwargs)\n\nFile /databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1750, in Module.\\_call\\_impl(self, \\*args, \\*\\*kwargs)1745 # If we don't have any hooks, we want to skip the rest of the logic in1746 # this function, and just call forward.1747 if not (self.\\_backward\\_hooks or self.\\_backward\\_pre\\_hooks or self.\\_forward\\_hooks or self.\\_forward\\_pre\\_hooks1748or \\_global\\_backward\\_pre\\_hooks or \\_global\\_backward\\_hooks1749or \\_global\\_forward\\_hooks or \\_global\\_forward\\_pre\\_hooks):\n\n\\-&gt; 1750return forward\\_call(\\*args, \\*\\*kwargs)1752 result = None1753 called\\_always\\_called\\_hooks = set()\n\nFile /local\\_disk0/.ephemeral\\_nfs/envs/pythonEnv-571e3abe-4219-4b3f-a998-7d01f4feeaa0/lib/python3.12/site-packages/peft/tuners/tuners\\_utils.py:222, in BaseTuner.forward(self, \\*args, \\*\\*kwargs)221 def forward(self, \\*args: Any, \\*\\*kwargs: Any):\n\n\\--&gt; 222return self.model.forward(\\*args, \\*\\*kwargs)\n\nFile /Workspace/Users/\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588/DataScience/ds\\_iteratie\\_28/\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588/ml\\_model\\_dev/notebooks/multimodaal/eda/modular\\_DS\\_001/understanding\\_tests\\_001/unsloth\\_compiled\\_cache/unsloth\\_compiled\\_module\\_gemma3.py:888, in Gemma3ForConditionalGeneration.forward(self, input\\_ids, pixel\\_values, attention\\_mask, position\\_ids, past\\_key\\_values, token\\_type\\_ids, cache\\_position, inputs\\_embeds, labels, use\\_cache, output\\_attentions, output\\_hidden\\_states, return\\_dict, logits\\_to\\_keep, \\*\\*lm\\_kwargs)870 def forward(871self,872input\\_ids: torch.LongTensor = None,(...)886\\*\\*lm\\_kwargs,887 ) -&gt; Union\\[tuple, Gemma3CausalLMOutputWithPast\\]:\n\n\\--&gt; 888return Gemma3ForConditionalGeneration\\_forward(self, input\\_ids, pixel\\_values, attention\\_mask, position\\_ids, past\\_key\\_values, token\\_type\\_ids, cache\\_position, inputs\\_embeds, labels, use\\_cache, output\\_attentions, output\\_hidden\\_states, return\\_dict, logits\\_to\\_keep, \\*\\*lm\\_kwargs)\n\nFile /Workspace/Users/\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588/DataScience/ds\\_iteratie\\_28/\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588/ml\\_model\\_dev/notebooks/multimodaal/eda/modular\\_DS\\_001/understanding\\_tests\\_001/unsloth\\_compiled\\_cache/unsloth\\_compiled\\_module\\_gemma3.py:795, in Gemma3ForConditionalGeneration\\_forward(self, input\\_ids, pixel\\_values, attention\\_mask, position\\_ids, past\\_key\\_values, token\\_type\\_ids, cache\\_position, inputs\\_embeds, labels, use\\_cache, output\\_attentions, output\\_hidden\\_states, return\\_dict, logits\\_to\\_keep, \\*\\*lm\\_kwargs)793if attention\\_mask is not None:794torch.\\_dynamo.mark\\_dynamic(attention\\_mask, 1)\n\n\\--&gt; 795loss = unsloth\\_fused\\_ce\\_loss(796trainer= None,797hidden\\_states= \\_hidden\\_states,798lm\\_head\\_weight= lm\\_head\\_weight,799lm\\_head\\_bias= lm\\_head\\_bias,800labels= labels,801mask= attention\\_mask,802n\\_items= n\\_items,803scaling= getattr(self, \"accelerator\\_scaler\", None),804target\\_gb= None,805torch\\_compile= not UNSLOTH\\_COMPILE\\_DISABLE,806logit\\_scale\\_multiply = () if () != () else 0,807logit\\_scale\\_divide= () if () != () else 0,808logit\\_softcapping= () if () != () else 0,809)812 if not return\\_dict:813output = (logits,) + outputs\\[1:\\]\n\nFile /local\\_disk0/.ephemeral\\_nfs/envs/pythonEnv-571e3abe-4219-4b3f-a998-7d01f4feeaa0/lib/python3.12/site-packages/unsloth\\_zoo/fused\\_losses/cross\\_entropy\\_loss.py:362, in unsloth\\_fused\\_ce\\_loss(trainer, hidden\\_states, lm\\_head\\_weight, lm\\_head\\_bias, labels, mask, n\\_items, scaling, target\\_gb, torch\\_compile, overwrite, \\*\\*kwargs)360 scaling = scaler.get\\_scale() if scaler is not None else scaling361 if hasattr(scaling, \"get\\_scale\"): scaling = scaling.get\\_scale()\n\n\\--&gt; 362 return apply\\_autograd\\_function(UnslothFusedLoss, dict(363loss\\_function = compute\\_fused\\_ce\\_loss,364hidden\\_states = hidden\\_states,365lm\\_head\\_weight = lm\\_head\\_weight,366lm\\_head\\_bias = lm\\_head\\_bias,367labels = labels,368mask = mask,369n\\_items = n\\_items,370scaling = scaling,371shift\\_labels = True,372target\\_gb = target\\_gb,373torch\\_compile = torch\\_compile,374overwrite = overwrite,375extra\\_kwargs = kwargs,376 ))\n\nFile /local\\_disk0/.ephemeral\\_nfs/envs/pythonEnv-571e3abe-4219-4b3f-a998-7d01f4feeaa0/lib/python3.12/site-packages/unsloth\\_zoo/fused\\_losses/cross\\_entropy\\_loss.py:41, in apply\\_autograd\\_function(autograd, mapping)39 def apply\\_autograd\\_function(autograd, mapping):40parameters, defaults = \\_get\\_mapping(autograd)\n\n\\---&gt; 41return getattr(autograd, \"apply\")(\\*(42mapping.get(old\\_key, default) \\\\43for old\\_key, default in zip(parameters, defaults)44))\n\nFile /databricks/python/lib/python3.12/site-packages/torch/autograd/function.py:575, in Function.apply(cls, \\*args, \\*\\*kwargs)572 if not torch.\\_C.\\_are\\_functorch\\_transforms\\_active():573# See NOTE: \\[functorch vjp and autograd interaction\\]574args = \\_functorch.utils.unwrap\\_dead\\_wrappers(args)\n\n\\--&gt; 575return super().apply(\\*args, \\*\\*kwargs)# type: ignore\\[misc\\]577 if not is\\_setup\\_ctx\\_defined:578raise RuntimeError(579\"In order to use an autograd.Function with functorch transforms \"580\"(vmap, grad, jvp, jacrev, ...), it must override the setup\\_context \"581\"staticmethod. For more details, please see \"582\" target=\"\\_blank\" rel=\"noopener noreferrer\"&gt;\n\nFile /local\\_disk0/.ephemeral\\_nfs/envs/pythonEnv-571e3abe-4219-4b3f-a998-7d01f4feeaa0/lib/python3.12/site-packages/unsloth\\_zoo/fused\\_losses/cross\\_entropy\\_loss.py:302, in UnslothFusedLoss.forward(ctx, loss\\_function, hidden\\_states, lm\\_head\\_weight, lm\\_head\\_bias, labels, mask, n\\_items, scaling, shift\\_labels, target\\_gb, torch\\_compile, overwrite, extra\\_kwargs)293accumulate\\_chunk = torch.compile(294accumulate\\_chunk,295dynamic = True,296fullgraph = True,297options = torch\\_compile\\_options,298)300 for (grad\\_inputs\\_j, hidden\\_states\\_j, labels\\_j,) in \\\\301zip(\\_\\_grad\\_inputs, \\_\\_shift\\_states, \\_\\_shift\\_labels,):\n\n\\--&gt; 302accumulate\\_chunk(303n\\_chunks = n\\_chunks,304grad\\_inputs\\_j = grad\\_inputs\\_j,305grad\\_lm\\_head = grad\\_lm\\_head,306grad\\_lm\\_head\\_bias = grad\\_lm\\_head\\_bias,307hidden\\_states\\_j = hidden\\_states\\_j,308lm\\_head\\_weight = lm\\_head\\_weight,309lm\\_head\\_bias = lm\\_head\\_bias,310labels\\_j = labels\\_j,311divisor = divisor,312scaling = scaling,313shift\\_labels = shift\\_labels,314\\*\\*extra\\_kwargs,315)316 pass317 ctx.save\\_for\\_backward(grad\\_inputs, grad\\_lm\\_head, grad\\_lm\\_head\\_bias)\n\nFile /databricks/python/lib/python3.12/site-packages/torch/\\_dynamo/eval\\_frame.py:577, in \\_TorchDynamoContext.\\_\\_call\\_\\_.&lt;locals&gt;.\\_fn(\\*args, \\*\\*kwargs)574return fn(\\*args, \\*\\*kwargs)575 finally:576# Restore the dynamic layer stack depth if necessary.\n\n\\--&gt; 577torch.\\_C.\\_functorch.pop\\_dynamic\\_layer\\_stack\\_and\\_undo\\_to\\_depth(578saved\\_dynamic\\_layer\\_stack\\_depth579)581\\_maybe\\_set\\_eval\\_frame(prior)582set\\_skip\\_guard\\_eval\\_unsafe(prior\\_skip\\_guard\\_eval\\_unsafe)", "link": "https://www.reddit.com/r/unsloth/comments/1nw44f4/runtimeerror_for_example_notebook_gemma3_4bvision/", "author": "None"}, {"title": "Manchester synagogue attack latest: police say suspect shot after four people injured by vehicle and stabbings", "summary": "", "link": "https://www.bbc.com/news/live/cx2703lnww4t", "author": "None"}, {"title": "A New, Transactional Era in US-India Relations", "summary": "", "link": "https://nationalinterest.org/blog/silk-road-rivalries/a-new-transactional-era-in-us-india-relations", "author": "None"}, {"title": "U.S. solar will pass wind in 2025 and leave coal in the dust soon after", "summary": "", "link": "https://pv-magazine-usa.com/2025/10/01/u-s-solar-will-pass-wind-in-2025-and-leave-coal-in-the-dust-soon-after/", "author": "None"}, {"title": "Israel orders Palestinians to leave Gaza City, saying those who stay will be considered militants", "summary": "", "link": "https://www.washingtonpost.com/world/2025/10/01/israel-palestinians-hamas-war-news-10-01-2025/c71ed44a-9eab-11f0-af12-ae28224a8694_story.html?itid=sr_3_ca46864c-1bfd-4779-a7c6-8673049b7887", "author": "None"}]}}, "Sport": {}};