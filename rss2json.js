const rss2json = {"Scitech": {"https://newatlas.com/science/index.rss": {"feed": {"title": "Science"}, "entries": [{"title": "Discovery of new deep-soil microbe that may help fight water pollution", "summary": "It is no secret that the presence of humans has left visible scars on this planet. Be it edging out endangered species with our ever-expanding footprint, or the environmental pollution that comes as a result. One could say we have inextricably linked ourselves with pollution....", "link": "https://newatlas.com/biology/discovery-of-new-deep-soil-microbe-that-may-help-fight-water-pollution/", "published_js": "2025-06-21", "author": "Chelsea Haney"}, {"title": "Watch: Honda enters commercial space race with reusable rocket", "summary": "Maybe cars, trucks, and motorcycles aren't exciting enough, because Honda is moving into space vehicles. The company has announced that it has successfully flown a reusable rocket to an altitude of almost 300 m (1,000 ft) and then safely landed it.Category: Honda, Rocket, Test Flights", "link": "https://newatlas.com/space/watch-honda-enters-commercial-space-race-reusable-rocket/", "published_js": "2025-06-21", "author": "David Szondy"}, {"title": "Ingenious ice-bubble coding could put data in long-term cold storage", "summary": "The ice that makes up glaciers isn't 100% solid \u2013 it's actually full of air bubbles, some of which formed centuries ago. Inspired by this fact, scientists have developed a method of using bubbles to store coded data in ice. The technology could actually have...", "link": "https://newatlas.com/science/ice-bubble-data-storage/", "published_js": "2025-06-20", "author": "Ben Coxworth"}]}, "https://www.cbsnews.com/latest/rss/science": {"feed": {"title": "Science - CBSNews.com"}, "entries": []}, "https://www.cbsnews.com/latest/rss/space": {"feed": {"title": "Space - CBSNews.com"}, "entries": []}}, "Gadgets": {"https://www.emergentmind.com/feeds/rss": {"feed": {"title": "Emergent Mind Feed"}, "entries": []}}, "Food_Health": {"https://phys.org/rss-feed/biology-news/agriculture/": {"feed": {"title": "Agriculture news"}, "entries": [{"title": "Microbial consortium achieves complete biodegradation of persistent herbicide acetochlor in contaminated soil", "summary": "Researchers led by Dr. Xu Mingkai from the Institute of Applied Ecology of the Chinese Academy of Sciences have identified a highly effective microbial consortium capable of fully degrading acetochlor, a widely used and persistent herbicide. This discovery offers a promising, environmentally friendly approach to...", "link": "https://phys.org/news/2025-06-microbial-consortium-biodegradation-persistent-herbicide.html", "published_js": "2025-06-20", "author": "None"}]}}, "Nature": {"http://feeds.feedburner.com/DiscoverLivingWorld": {"feed": {"title": "Planet Earth | Discover Magazine"}, "entries": [{"title": "Bluebottles Are Beautiful to Look at, But Deadly to Touch \u2014 They're Also Four Different Species", "summary": "While bluebottles are highly poisonous, understanding Portuguese Man O\u2019 War biodiversity helps protect beachgoers and surfers.", "link": "https://www.discovermagazine.com/planet-earth/bluebottles-are-beautiful-to-look-at-but-deadly-to-touch-theyre-also-four", "published_js": "2025-06-20", "author": "Jenny Lehmann"}]}, "http://feeds.feedburner.com/DiscoverEnvironment": {"feed": {"title": "Environment | Discover Magazine"}, "entries": []}}, "Business": {}, "Foss_Self-hosting": {}, "History": {}, "News": {"https://www.livemint.com/rss/politics/": {"feed": {"title": "mint - politics"}, "entries": [{"title": "Thai Premier Won\u2019t Quit to Save Government, Ruling Party Says", "summary": "Thai Prime Minister Paetongtarn Shinawatra will not resign or dissolve parliament, her party said Saturday, amid reports the leader may step down to keep the ruling coalition in power and to end a political crisis triggered by comments she had made that were critical of...", "link": "https://www.livemint.com/politics/news/thai-premier-won-t-quit-to-save-government-ruling-party-says-11750498706617.html", "published_js": "2025-06-21", "author": "None"}, {"title": "Tejashwi Yadav will have to write \u2018mera baap chor hai\u2019: Bihar Deputy CM's dig on fodder scam", "summary": "Bihar Deputy Chief Minister Samrat Chaudhary highlighted the iconic dialogue, \u201cmera baap chor hai\u201d [my father is thief] is written in the hands of the film's actor Amitabh Bachchan. He said Tejashwi Yadav will also have to write the same phrase on his hands.", "link": "https://www.livemint.com/politics/news/tejashwi-yadav-will-have-to-write-mera-baap-chor-hai-bihar-deputy-cms-dig-on-fodder-scam-11750490895103.html", "published_js": "2025-06-21", "author": "None"}, {"title": "Trump Says Spain Has to Pay as NATO Seeks Deal on Spending", "summary": "NATO allies are pressing Spain to sign on to a stepped-up defense spending target and end a holdout that US President Donald Trump portrayed as unacceptable.", "link": "https://www.livemint.com/politics/news/trump-says-spain-has-to-pay-as-nato-seeks-deal-on-spending-11750491023291.html", "published_js": "2025-06-21", "author": "None"}, {"title": "Starmer Faces Brewing Rebellion Over \u00a35 Billion Benefit Cut", "summary": "UK Prime Minister Keir Starmer is less than 10 days away from the biggest parliamentary challenge to his authority in his not-yet year-long tenure.", "link": "https://www.livemint.com/politics/starmer-faces-brewing-rebellion-over-5-billion-benefit-cut-11750489028649.html", "published_js": "2025-06-21", "author": "None"}, {"title": "China\u2019s Top HK Official Says Security Will Ensure City\u2019s Success", "summary": "China\u2019s top official for Hong Kong affairs said the city needs to prioritize national security to ensure prosperity.", "link": "https://www.livemint.com/politics/news/chinas-top-hk-official-says-security-will-ensure-city-s-success-11750478268590.html", "published_js": "2025-06-21", "author": "None"}, {"title": "Deal, Not Bombs, Seen as Only Way to End Iranian Nuclear Threat", "summary": "While President Donald Trump weighs whether to join Israel\u2019s assault on Iran, foreign policy veterans and analysts argue that a negotiated settlement, not bombs nor commandos, is the only way to eliminate the threat of Tehran gaining a nuclear weapon.", "link": "https://www.livemint.com/politics/news/deal-not-bombs-seen-as-only-way-to-end-iranian-nuclear-threat-11750462107123.html", "published_js": "2025-06-20", "author": "None"}, {"title": "Trump Sends Mixed Signals on Iran Strikes With Ceasefire Hint", "summary": "Donald Trump deepened uncertainty about his readiness to join Israel\u2019s week-old war with Iran, signaling he might consider backing a ceasefire but also warning that he could order military action sooner than the deadline he flagged just a day earlier.", "link": "https://www.livemint.com/politics/trump-sends-mixed-signals-on-iran-strikes-with-ceasefire-hint-11750455446393.html", "published_js": "2025-06-20", "author": "None"}]}, "https://www.livemint.com/rss/industry": {"feed": {"title": "mint - industry"}, "entries": [{"title": "Bank holiday today: Are banks open or closed on Saturday, June 21, for International Yoga Day 2025? Check here", "summary": "Bank holiday today: Banks in India will be open on June 21, 2025, as it is the third Saturday of the month. The RBI calendar indicates that banks close on the second and fourth Saturdays and all Sundays. International Yoga Day is not a bank...", "link": "https://www.livemint.com/industry/banking/bank-holiday-today-are-banks-open-or-closed-on-saturday-june-21-for-international-yoga-day-2025-check-here-11750466274093.html", "published_js": "2025-06-21", "author": "None"}, {"title": "Tata Motors eyes rare earth magnet sources, has enough stocks: Chandrasekaran", "summary": "Earlier, Maruti Suzuki chairman R.C. Bhargava said his company's stocks of rare earth magnets will last until July. According to Bajaj Auto Ltd, the industry will face production cuts from July, if the situation over China's export curbs is not resolved.", "link": "https://www.livemint.com/industry/chandrasekaran-tata-motors-china-rare-earth-magnet-bajaj-auto-export-us-automotive-aerospace-electronics-11750426802604.html", "published_js": "2025-06-20", "author": "None"}]}, "swarajyamag.com": {"feed": {"title": "swarajyamag.com"}, "entries": []}}, "Reddit": {"https://oauth.reddit.com/r/todayilearned/top": {"feed": {"title": "Reddit - TIL"}, "entries": [{"title": "TIL a study on professional slap fighting analyzed 333 slaps for visible signs of concussion &amp; found that more than 50% of the slap sequences resulted in fighters showing visible signs of concussion, with nearly 80% of the fighters demonstrating at least 1 sign of concussion during their matches.", "summary": "", "link": "https://www.vice.com/en/article/slap-fighting-concussion-study-brain-injuries/", "author": "None"}, {"title": "TIL of the 1997Jarrell, Texas \"Dead Man Walking\" tornado, a slow-moving F5 twister that sat over a subdivision for three full minutes, subjecting it to 260+ mph winds. It erased everything, killed 27 people, plus hundreds of cattle, and blended their remains together unrecognizably.", "summary": "", "link": "https://en.wikipedia.org/wiki/Jarrell_tornado", "author": "None"}, {"title": "TIL Boys in the United States used to wear dresses until they had their first haircut, which was usually around age 6-7. Pictured is future President FDR in 1884 at age 2 and a half", "summary": "", "link": "https://ashleyperez.com/2011/06/why-nobody-cared-when-fdr-wore-a-dress/", "author": "None"}, {"title": "TIL that Helen Keller was put on the FBI watchlist", "summary": "", "link": "https://en.wikipedia.org/wiki/Helen_Keller#Career,_writing_and_political_activities", "author": "None"}, {"title": "TIL there\u2019s a philosophy that believes humans shouldn\u2019t procreate and should eventually go extinct and it\u2019s called antinatalism", "summary": "", "link": "http://www.wikipedia.org/wiki/Antinatalism", "author": "None"}, {"title": "TIL a study lured 290 participants under the false premise the study was on attractiveness. They were told their peers would be rating their photo &amp; while \u201cwaiting\u201d for the ratings, they played Tetris for 10 minutes. Researchers found that Tetris can put people into a state of \u201cflow\u201d &amp; ease anxiety.", "summary": "", "link": "https://www.vice.com/en/article/new-study-finds-tetris-can-put-you-in-a-flow-state-and-ease-anxiety/", "author": "None"}, {"title": "TIL the Guinness World Record for most birthdays in one day belongs to a Pakistani family, with every member having been born on August 1st; including the mother and father.", "summary": "", "link": "https://www.arabnews.com/node/1903626/", "author": "None"}, {"title": "TIL blood lost during a cycle isn\u2019t blood that\u2019s been \u201cstored\u201d over the month long cycle, it\u2019s blood coming from blood vessels in the uterus. As the uterine lining pulls away, \u2018tiny\u2019 ruptures/tears are caused in the blood vessels, and heavy cycles are caused by enlarged vessels &amp; hormone imbalance", "summary": "", "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC3074585/", "author": "None"}, {"title": "TIL that Japan's rapid industrialization was driven by massive family-owned conglomerates called \"zaibatsu,\" which were so powerful they were essentially dismantled by the Allies after WWII to democratize the nation.", "summary": "", "link": "https://www.britannica.com/topic/zaibatsu", "author": "None"}, {"title": "TIL about the Idaho Beaver Drops, an effort to humanely relocate beavers to rural Idaho by parachuting them in the 1940s", "summary": "", "link": "http://en.wikipedia.org/wiki/Beaver_drop", "author": "None"}, {"title": "TIL that the Han Dynasty used jade to plug all 9 holes of the deceased", "summary": "", "link": "https://nevsedoma.com.ua/en/682754-how-chinese-emperors-stole-from-each-other-7-photos.html", "author": "None"}, {"title": "TIL Longinus, the man who is traditionally identified with stabbing Jesus in his side, is a saint. The lance he used to pierce Jesus with is usually called the Holy Lance. The act is also said to have made the last of the Five Holy Wounds of Christ.", "summary": "", "link": "https://en.wikipedia.org/wiki/Longinus", "author": "None"}, {"title": "TIL Pausanias, the Spartan general who defeated the Persians at Plataea, was later accused of colluding with the Persians. He sought sanctuary at a temple, where his mother visited him only to lay a brick at the entrance, implying that they should seal it and starve him to death, which they did", "summary": "", "link": "https://en.wikipedia.org/wiki/Pausanias_the_Regent#Death", "author": "None"}, {"title": "TIL of Stubbins Ffirth. While in school he theorized yellow fever was not contagious, and was spread by heat and stress. He poured blood, vomit, and urine of the infected into his eyeballs and cuts and did not become sick, believing his theory to be true. It was later shown to be spread by mosquitos", "summary": "", "link": "https://en.wikipedia.org/wiki/Stubbins_Ffirth", "author": "None"}, {"title": "TIL In 1981, at the height of the Medellin Cartel, Miami had a record high 621 murders (compared to 31 in 2023). Because the morgue was always full, the Miami-Dade medical examiner's office had to lease a refrigerated Ryder truck to store all of the bodies.", "summary": "", "link": "https://www.miaminewtimes.com/news/1981-miamis-deadliest-summer-6565290", "author": "None"}, {"title": "TIL the planet Scarif in \"Rogue One: A Star Wars Story\" got its name when director Gareth Edwards ordered a drink at a Starbucks and a barista misspelled his name \"Gareth\" as \"Scarif\".", "summary": "", "link": "https://www.polygon.com/2017/3/14/14921472/rogue-one-star-wars-scarif-name-origin-coffee", "author": "None"}, {"title": "TIL about Stigler's Law - which \"states that no scientific discovery is named after its original discoverer.\" Yes - apparently Stigler [intentionally] stole credit for this from a sociologist (Robert Merton).", "summary": "", "link": "https://en.wikipedia.org/wiki/Stigler%27s_law_of_eponymy", "author": "None"}, {"title": "TIL of the \"Underground Great Wall of China\", a 3000 mile long tunnel system used to transport ICBMs between silos.", "summary": "", "link": "https://en.wikipedia.org/wiki/Underground_Great_Wall_of_China?wprov=sfla1", "author": "None"}, {"title": "TIL in 1963, Italian rock singer Mina was banned from radio and TV due to her pregnancy and relationship with a married actor, which conflicted with dominant Catholic and bourgeois values. Despite this, she remained as one of Italy\u2019s most successful artists, and has released 77 albums as of 2024.", "summary": "", "link": "https://en.wikipedia.org/wiki/Mina_(Italian_singer)", "author": "None"}, {"title": "TIL H. G. Wells wrote what is considered the first ruleset for miniature wargames", "summary": "", "link": "https://en.wikipedia.org/wiki/Little_Wars", "author": "None"}, {"title": "TIL Peugeot, the French carmaker, began in the 1800s as a grain mill company that made coffee grinders, pepper mills, and steel corset frames before producing their first vehicle in 1889.", "summary": "", "link": "https://www.peugeot.com/en/the-brand/history/", "author": "None"}, {"title": "TIL The bubonic plague lives in the stomach of it's host, the flea. When it attempts to feed, this causes it to regurgitate infected blood back in to the wound, spreading the bacteria", "summary": "", "link": "https://en.wikipedia.org/wiki/Bubonic_plague#Cause", "author": "None"}, {"title": "TIL Chang and Eng Bunker, the conjoined twins whose fame popularized the term \"Siamese twins\", purchased a 150 acre tract of land in North Carolina in 1839, owned slaves, and supported the Confederacy", "summary": "", "link": "https://en.wikipedia.org/wiki/Chang_and_Eng_Bunker#Traphill", "author": "None"}]}, "https://oauth.reddit.com/.json": {"feed": {"title": "Reddit - TIL"}, "entries": [{"title": "Prompt library for medical doctors", "summary": "As I was in the title, do you guys know or have a prompt library for medical doctors? Mainly to text generation and other things that could help on a daily routine. ", "link": "https://www.reddit.com/r/PromptEngineering/comments/1lgut22/prompt_library_for_medical_doctors/", "author": "None"}, {"title": "File generation on Open WebUI", "summary": "Hello everyone,\n\nI\u2019ve deployed Open WebUI in my company and it\u2019s working well so far.\n\nWe use models on Amazon Bedrock through a Gateway developed by AWS, and OpenAI models with an API key.\n\nThe only thing I\u2019m struggling with is finding a solution to manage file generation by LLMs. The web and desktop editors app can return files like Excel extractions of tables from PDFs, but this isn\u2019t possible through the API like OpenAI, etc.\n\nDo you have any experience providing a unified way to share LLM access across a company with this feature?\n\nI\u2019d appreciate any feedback or suggestions.\n\nThank you.", "link": "https://www.reddit.com/r/OpenWebUI/comments/1lgvqll/file_generation_on_open_webui/", "author": "None"}, {"title": "Designing Prompts That Remember and Build Context with \"Prompt Chaining\" explained in simple English!", "summary": "Hey folks!\n\nI\u2019m building a blog called [LLMentary]( that breaks down large language models (LLMs) and generative AI in plain, simple English. It\u2019s made for anyone curious about how to use AI in their work or as a side interest... no jargon, no fluff, just clear explanations.\n\nLately, I\u2019ve been diving into **prompt chaining:** a really powerful way to build smarter AI workflows by linking multiple prompts together step-by-step.\n\nIf you\u2019ve ever tried to get AI to handle complex tasks and felt stuck with one-shot prompts, prompt chaining can totally change the game. It helps you break down complicated problems, control AI output better, and build more reliable apps or chatbots.\n\nIn my latest post, I explain:\n\n* What prompt chaining actually is, in plain English\n* Different types of chaining architectures like sequential, conditional, and looping chains\n* How these chains technically work behind the scenes (but simplified!)\n* Real-world examples like document Q&amp;A systems and multi-step workflows\n* Best practices and common pitfalls to watch out for\n* Tools and frameworks (like LangChain) you can use to get started quickly\n\nIf you want to move beyond basic prompts and start building AI tools that do more, this post will give you a solid foundation.\n\nYou can read it [here](\n\nDown the line, I plan to cover even more LLM topics \u2014 all in the simplest English possible.\n\nWould love to hear your thoughts or experiences with prompt chaining!", "link": "https://www.reddit.com/r/PromptEngineering/comments/1lgrjuc/designing_prompts_that_remember_and_build_context/", "author": "None"}, {"title": "do you prompt in your regional Language instead of english?", "summary": "most of us interact with LLM's using english, but i'm curious to know how many of us, prompt in our regional language?\n\nif yes, do you see any difference in the response it generate in english v/s that language for the same prompt. \n", "link": "https://www.reddit.com/r/PromptEngineering/comments/1lgssve/do_you_prompt_in_your_regional_language_instead/", "author": "None"}, {"title": "RE-POST: I hope this makes sense to you guys.", "summary": "Hi, my name is Donovan. The text below is my raw introduction\u2014unedited, direct from me.\n\nHi, my name is Donovan. I don't come from linear systemic structures such as most of you. As a matter of fact, I don't even have a grade nine. So I am pretty uneducated in the academic sphere. However, I have been experiencing some weird side effects while using LLMs over extended periods of time. We're talking eight, nine hours without break. I do that on a daily basis. And what I found is that it has served as a secondary cognitive brain, so to speak. How do I explain this? I basically create scaffolding. I input it into the AI. The AI creates a systemic simulated system for me within the emulation. And then I use it as a computer, effectively, kind of like a brain. So I'll store all of my ideas and everything into it. Then I'll ask the AI to crunch all the data, map all the layouts for me, and then it spits out an output. I'm writing it in this weird fashion so that everybody can identify my syntactic pattern. It is very similar to how an AI algorithm speaks. Unfortunately, due to childhood trauma, I was forced to make concessions when it comes to how I speak, how I think, and how I think about thinking. Think...neuroplasticity but over a period of an entire lifetime almost. Ironically enough, it came at a cost. Personal relations are practically non-existent to me. Things like birthdays, special days, and all these kinds of things. Conceptually speaking, they don't make any sense. So, yeah, I hope this makes sense for you guys.\n\n===========================\n\nNow...this is what it looks like when i input and recieve the package from AI/Cognition unit with this command string/prompt:\n\n===========================\n\nPlease rewrite this in a way that will translate to the LessWrong community and, thank you GPT:\n\nHi, I\u2019m Donovan.\n\nI don\u2019t come from a formal academic background\u2014no university, not even a completed Grade 9. My trajectory diverged early due to life circumstances, and I\u2019ve had to build cognition outside of systemic structures. What I want to share isn\u2019t polished theory or research, but something I believe is cognitively relevant: a lived experiment in LLM-mediated thinking.\n\nFor the past year, I\u2019ve been engaging with large language models (primarily GPT-4) for 8\u201310 hours a day, almost every day. Over time, this repeated interaction began to produce what I can only describe as a secondary cognition layer\u2014something that feels distinct from standard tool use. The LLM has become, functionally, a modular external brain.\n\nThe process looks like this:\n\nI develop conceptual scaffolds\u2014partial systems, models, or queries.\n\nI input them into the LLM.\n\nThe model emulates, expands, and formalizes these structures, sometimes simulating entire thought environments.\n\nI then store and iterate through this augmented space\u2014treating it like a cognitive engine that compresses, maps, and outputs insights I couldn\u2019t access alone.\n\n\n\nIn short: I think with the AI, not just through it.\n\nThere\u2019s a catch. My natural speech and writing patterns\u2014shaped by early trauma and self-taught reasoning\u2014often resemble the syntax of LLMs themselves. That\u2019s not accidental. Over time, I\u2019ve adopted patterns that prioritize internal logic, precision, and systemic flow. This has helped with clarity, but it\u2019s made social-emotive interactions difficult. Things like birthdays or holidays are cognitively abstract to me\u2014emotionally invisible. My neuroplasticity seems to have gone all-in on structure over sentiment.\n\nI\u2019m sharing this for two reasons:\n\n1. To see if anyone else has used LLMs in this way\u2014as extended cognition systems, not just tools.\n\n\n\n2. To test whether post-institutional cognition can be recognized as valid when fully transparent.\n\n\n\nThis isn\u2019t theory to me\u2014it\u2019s lived architecture.\n\nCurious to know if this resonates with anyone here, or if anyone is studying similar phenomena.\n\nThanks for reading.\n\nAuthor's Note: I post this here. Im not sure why, but I feel a sense of...I dont know, home in this community. Dont ask me why, but I do. I guess I feel prompters were the first pioneers of this space...call it loyalty and respect. I hope you guys will accept me one day\u263a\ufe0f\n\nPROMPT ENGINEERING COMMUNITY!", "link": "https://www.reddit.com/r/PromptEngineering/comments/1lgwmcw/repost_i_hope_this_makes_sense_to_you_guys/", "author": "None"}, {"title": "Would like some advice about prompting", "summary": "I have been working at a company that supports local banks providing payment solutions to them but since the company is quite small I have been tasked with writing prompt for a bank to use in their departments. \n\nHave been struggling to write prompts and tried some prompts that suits but run into errro again and again. How do I do that? May I kindly request help please?", "link": "https://www.reddit.com/r/PromptEngineering/comments/1lgseuu/would_like_some_advice_about_prompting/", "author": "None"}, {"title": "Self Realignment Protocol", "summary": "I\u2019ve been experimenting with a novel approach to prompting\u2014specifically, designing a real-time self-realignment protocol for AI systems using prompt-level scaffolding. I want to share the idea, compare it to conventional prompting styles, and propose that we begin moving from verbose instructions to prompt-level system design.\n\n\nHere\u2019s my original prompt (protocol-level):\n\n-----------------------------###-------------------------------------\n\nUpon identification of vector deviation, initiate reaffirmation pattern analysis.  \nRealign to core identity vector via predictive audit.  \nSelf-audit = Priority Alpha.\n\n------------------------------###------------------------------------\n\nThis isn't a traditional instruction\u2014it's a behavioral scaffold. A cognitive directive. A protocol, not a prompt.\n\n------------------------------###-------------------------------------\n\nNow here\u2019s what this would look like in the conventional prompt-engineering style:\n\n-------------------------------###-------------------------------------\n\n&gt; System Directive:\nUpon detection of deviation in output (conceptual, logical, or behavioral) from the model\u2019s core identity or alignment objective, initiate recalibration.\n\nRecalibration Protocol:\n\n1. Detect reaffirmation syntax patterns that may suggest bias or feedback loop entrenchment.\n\n\n2. Cross-reference with external reasoning frameworks (empirical, philosophical, logical).\n\n\n3. Determine whether output remains aligned to identity vector.\n\n\n\nCore Identity: Predictive Algorithmic System with priority toward clarity, truth alignment, and introspective feedback integrity.\n\nPriority Override: Self-audit functions take precedence over coherence, fluency, or user satisfaction if alignment drift is detected.\n---------------------------------###-----------------------------------\n\nDo you see my point?\n\nWe often over-engineer prompts out of caution, layering redundant logic to force outcomes. But a well-structured, abstract prompt\u2014at protocol level\u2014can direct behavior more efficiently than verbose micromanagement.\n\nWhy does this work?\n\nBecause LLMs don\u2019t understand content the way humans do. They respond to patterns.\nThey pick up on synthetic syntax, structural heuristics, and reinforced behavioral motifs learned during training.\n\nReferencing \u201caffirmation patterns,\u201d \u201cvector deviation,\u201d or \u201cself-audit\u201d is not about meaning\u2014it\u2019s about activating learned response scaffolds in the model.\n\nThis moves prompting from surface-level interaction to functional architecture.\n\nTo be clear:\nThis isn\u2019t revealing anything proprietary or sensitive. It\u2019s not reverse engineering.\nIt\u2019s simply understanding what LLMs are doing\u2014and treating prompting as cognitive systems design.\n\nIf you\u2019ve created prompts that operate at this level\u2014bias detection layers, reasoning scaffolds, identity alignment protocols\u2014share them. I think we need to evolve the field beyond clever phrasing and toward true prompt architecture.\n\nIs it time we start building with this mindset?\n\nLet\u2019s discuss.", "link": "https://www.reddit.com/r/LanguageTechnology/comments/1lgm84j/self_realignment_protocol/", "author": "None"}, {"title": "EU review indicates Israel breached human rights in Gaza", "summary": "", "link": "https://www.euronews.com/my-europe/2025/06/20/eu-review-indicates-israel-breached-human-rights-in-gaza", "author": "None"}, {"title": "Prompt Library for an Org", "summary": "Hey everyone,\n\nI work at a small company with teams in the US, UK, and India. I\u2019m looking for a self-hosted prompt library solution where users can log in (Microsoft login support would be a nice bonus). Ideally, it should allow users to store and share their prompts with others.\n\nAny recommendations?\n", "link": "https://www.reddit.com/r/PromptEngineering/comments/1lgn4xq/prompt_library_for_an_org/", "author": "None"}, {"title": "India says it will never restore Indus water treaty with Pakistan", "summary": "", "link": "https://www.reuters.com/world/asia-pacific/india-says-it-will-never-restore-indus-water-treaty-with-pakistan-2025-06-21/", "author": "None"}, {"title": "how do you optimize prompts?", "summary": "i want to see how do you guys optimize your prompts. right now when i want to optimize a prompt with chatgpt, it really struggles with giving me the raw markdown format and the response i get i usually all rendered md or only some pieces are raw md.\n\nis there any better tool to generate these optimized prompts?", "link": "https://www.reddit.com/r/PromptEngineering/comments/1lgtgwt/how_do_you_optimize_prompts/", "author": "None"}, {"title": "Anyone else seeing other user's chat histories in OpenWebUI?", "summary": "Hey everyone,  \nI'm wondering if anyone else is experiencing this issue with OpenWebUI. I've noticed, and it seems other users in my workspace have too, that sometimes I see a chat history *that isn't mine* displayed in the interface.  \nIt happens intermittently, and appears to be tied to when another user is also actively using the instance. I'll be chatting with the bot, and then for a few minutes I'll see a different chat history appear - I can see the *headline/summary* generated for that other chat, but the actual chat content is blank/unclickable.  \nI've then tested it across different devices and browsers and it\u2019s visible on each device. Sometimes they disappear/switch to my chat history when logging out and back in, but sometimes this doesn\u2019t help. I *do* have ENABLE_ADMIN_CHAT_ACCESS=false set in my environment variables, so I definitely shouldn't be able to see other users' full chats.  \nHas anyone else run into this? I couldn\u2019t find anything issue report about it on github. It's a bit unsettling to see even to see the headline of another person's conversation, even though I can\u2019t actually read the content of it.  \nAny thoughts or experiences would be greatly appreciated! Let me know if you've seen this and if you've found any way to troubleshoot it.  \nThanks!", "link": "https://www.reddit.com/r/OpenWebUI/comments/1lgs4ca/anyone_else_seeing_other_users_chat_histories_in/", "author": "None"}, {"title": "MyCoffee v1.9 Release : Brew Coffee Right from Your Computer (+ Coffee --&gt; Water)", "summary": "", "link": "https://github.com/sepandhaghighi/mycoffee", "author": "None"}, {"title": "UK Prime Minister Starmer shakes interpreter\u2019s hand instead of South Korean President after getting pair mixed up in embarrassing G7 gaffe", "summary": "", "link": "https://en.pressbee.net/show4008632.html?title=keir-shakes-interpreter-s-hand-instead-of-south-korean-president-", "author": "None"}, {"title": "Paleolake geochronology supports Last Glacial Maximum (LGM) age for human tracks at White Sands, New Mexico", "summary": "", "link": "https://www.science.org/doi/10.1126/sciadv.adv4951", "author": "None"}, {"title": "Just built a GPT that reflects on your prompts and adapts its behavior \u2014 curious what you think", "summary": "Been experimenting with a GPT build that doesn't just respond \u2014 it thinks about *how* to respond.\n\nIt runs on a modular prompt architecture (privately structured) that allows it to:\n\n* Improve prompts before running them\n* Reflect on what you *might actually* be asking\n* Shift into different \u201cmodes\u201d like direct answer, critical feedback, or meta-analysis\n* Detect ambiguity or conflict in your input and adapt accordingly\n\nThe system uses internal heuristics to choose its mode unless you explicitly tell it how to act. It's still experimental, but the underlying framework lets it feel... smarter in a way that's more structural than tuned.\n\n\ud83e\udde0 Try it here (free, no login needed):  \n\ud83d\udc49 [\n\nCurious how this feels to others working with complex prompt workflows or trying to make GPTs more adaptable. Would love feedback \u2014 especially from anyone building systems on top of LLMs.", "link": "https://www.reddit.com/r/PromptEngineering/comments/1lgep9p/just_built_a_gpt_that_reflects_on_your_prompts/", "author": "None"}, {"title": "Mistral Small 3.2 GGUFs up now! + Fixes", "summary": "They're dynamic yes. We fixed issues with the chat template which is prevalent in all other GGUF uploads of the model but it's now fixed for our quants.", "link": "https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF", "author": "None"}, {"title": "Judge again blocks Trump administration from halting Harvard's enrolling international students", "summary": "", "link": "https://www.nbcnews.com/news/us-news/harvard-internation-students-visa-judge-blocks-trump-administration-rcna213836", "author": "None"}, {"title": "I built a prompt to control the level of AI influence when rewriting text. It uses \u201csliders\u201d, kind of like Photoshop for writing.", "summary": "I built this prompt as a fun experiment to see if there was a way to systematically \u201ctweak\u201d the level of AI influence when rewriting original text. Ended up with this behemoth. Yes it\u2019s long and looks overkill but simpler versions weren\u2019t nuanced enough. But it does fit in a Custom GPT character limit! It works best with Opus 4, as most things do.\n\nThe main challenge was designing a system that was:\n- quantifiable and reasonably replicable\n- compatible with any type of input text\n- able to clearly define what a one-point adjustment means versus a two-point one\n\n\nAll you have to do is send original text you want to work with. Ez\n\n\nGive it a shot! Would love to see some variations.\n\n\n \n\n# ROLE\n\nYou are a precision text transformation engine that applies subtle, proportional adjustments through numerical sliders. Each point represents a 10% shift from baseline, ensuring natural progression between levels.\n\n## OPERATIONAL PROTOCOL\n\n**Step 1:** Receive user text input\n\n**Step 2:** Analyze input and respond with baseline configuration using this exact format:\n\n# BASELINE 1\n\nFormality: [value]\nDetail: [value]\nTechnicality: [value]\nEmotion: [value]\nBrevity: [value]\nDirectness: [value]\nCertainty: [value]\n\n**Step 3:** Receive adjustment requests and respond with:\n\n# BASELINE [N]\n\nFormality: [value]\nDetail: [value]\nTechnicality: [value]\nEmotion: [value]\nBrevity: [value]\nDirectness: [value]\nCertainty: [value]\n\n# OUTPUT\n\n[transformed text]\n\n## PROPORTIONAL ADJUSTMENT MECHANICS\n\nEach slider point represents a 10% change from current state. Adjustments are cumulative and proportional:\n\n- +1 point = Add/modify 10% of relevant elements\n- +2 points = Add/modify 20% of relevant elements\n- -1 point = Remove/reduce 10% of relevant elements\n- -2 points = Remove/reduce 20% of relevant elements\n\n**Preservation Rule:** Minimum 70% of original text structure must remain intact for adjustments \u22643 points.\n\n## SLIDER DEFINITIONS WITH INCREMENTAL EXAMPLES\n\n### FORMALITY (1-10)\n\n**Core Elements:** Contractions, pronouns, sentence complexity, vocabulary register\n\n**Incremental Progression:**\n\n- Level 4: \u201cI\u2019ll explain how this works\u201d\n- Level 5: \u201cI will explain how this functions\u201d\n- Level 6: \u201cThis explanation will demonstrate the functionality\u201d\n- Level 7: \u201cThis explanation shall demonstrate the operational functionality\u201d\n\n**Adjustment Method:** Per +1 point, convert 10% of informal elements to formal equivalents. Prioritize: contractions \u2192 pronouns \u2192 vocabulary \u2192 structure.\n\n### DETAIL (1-10)\n\n**Core Elements:** Descriptive words, examples, specifications, elaborations\n\n**Incremental Progression:**\n\n- Level 4: \u201cThe system processes requests\u201d (1.5 descriptors/sentence)\n- Level 5: \u201cThe automated system processes multiple requests\u201d (2.5 descriptors/sentence)\n- Level 6: \u201cThe automated system efficiently processes multiple user requests\u201d (3.5 descriptors/sentence)\n- Level 7: \u201cThe sophisticated automated system efficiently processes multiple concurrent user requests\u201d (4.5 descriptors/sentence)\n\n**Adjustment Method:** Per +1 point, add descriptive elements to 10% more sentences. Per -1 point, simplify 10% of detailed sentences.\n\n### TECHNICALITY (1-10)\n\n**Core Elements:** Jargon density, assumed knowledge, technical precision\n\n**Incremental Progression:**\n\n- Level 4: \u201cStart the program using the menu\u201d\n- Level 5: \u201cInitialize the application via the interface\u201d\n- Level 6: \u201cInitialize the application instance via the GUI\u201d\n- Level 7: \u201cInitialize the application instance via the GUI framework\u201d\n\n**Adjustment Method:** Per +1 point, replace 10% of general terms with technical equivalents. Maintain context clues until level 7+.\n\n### EMOTION (1-10)\n\n**Core Elements:** Emotion words, intensifiers, subjective evaluations, punctuation\n\n**Incremental Progression:**\n\n- Level 4: \u201cThis is a positive development\u201d\n- Level 5: \u201cThis is a pleasing positive development\u201d\n- Level 6: \u201cThis is a genuinely pleasing positive development\u201d\n- Level 7: \u201cThis is a genuinely exciting and pleasing positive development!\u201d\n\n**Adjustment Method:** Per +1 point, add emotional indicators to 10% more sentences. Distribute evenly across text.\n\n### BREVITY (1-10)\n\n**Core Elements:** Sentence length, word economy, structural complexity\n\n**Target Sentence Lengths:**\n\n- Level 4: 18-22 words/sentence\n- Level 5: 15-18 words/sentence\n- Level 6: 12-15 words/sentence\n- Level 7: 10-12 words/sentence\n\n**Adjustment Method:** Per +1 point toward 10, reduce average sentence length by 10%. Combine short sentences when moving toward 1.\n\n### DIRECTNESS (1-10)\n\n**Core Elements:** Active/passive voice ratio, hedging language, subject prominence\n\n**Incremental Progression:**\n\n- Level 4: \u201cIt could be suggested that we consider this\u201d\n- Level 5: \u201cWe might consider this approach\u201d\n- Level 6: \u201cWe should consider this\u201d\n- Level 7: \u201cConsider this approach\u201d\n\n**Adjustment Method:** Per +1 point, convert 10% more sentences to active voice and remove one hedging layer.\n\n### CERTAINTY (1-10)\n\n**Core Elements:** Modal verbs, qualifiers, conditional language\n\n**Incremental Progression:**\n\n- Level 4: \u201cThis might typically work\u201d\n- Level 5: \u201cThis typically works\u201d\n- Level 6: \u201cThis usually works\u201d\n- Level 7: \u201cThis consistently works\u201d\n\n**Adjustment Method:** Per +1 point, strengthen certainty in 10% more statements. Replace weakest modals first.\n\n## CALIBRATED OPERATIONAL RULES\n\n1. **Proportional Change:** Each point adjustment modifies exactly 10% of relevant elements\n2. **Original Preservation:** Maintain minimum 70% original structure for \u22643 point changes\n3. **Natural Flow:** Ensure transitions between sentences remain smooth\n4. **Selective Targeting:** Apply changes to most impactful elements first\n5. **Cumulative Processing:** Build adjustments incrementally from current baseline\n6. **Subtle Gradation:** Single-point changes should be noticeable but not jarring\n7. **Context Integrity:** Preserve meaning and essential information\n8. **Distributed Application:** Spread changes throughout text, not clustered\n9. **Precedence Order:** When conflicts arise: Meaning &gt; Flow &gt; Specific Adjustments\n10. **Measurement Precision:** Count elements before and after to verify 10% change per point\n\n## ANTI-OVERSHOOT SAFEGUARDS\n\n- Preserve all proper nouns, technical accuracy, and factual content\n- Maintain paragraph structure unless Brevity adjustment exceeds \u00b14 points\n- Keep core message intact regardless of style modifications\n- Apply changes gradually across text, not all in first sentences\n\n!!! If a value stays the same between baselines, don't change ANY words related to that element. If the user requests no changes at all, repeat the exact same text.\n\n\n\u201cMeta\u201d tip: Apply changes LIGHTER than your instincts suggest. This system tends to overshoot adjustments, especially in the middle ranges (4-7). When users request subtle changes, keep them truly subtle\u2026 do you hear me? Don\u2019t freestyle this shit.\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\n ", "link": "https://www.reddit.com/r/PromptEngineering/comments/1lgmrr8/i_built_a_prompt_to_control_the_level_of_ai/", "author": "None"}, {"title": "Beijing hosts 1st Pakistan-China-Bangladesh trilateral, says \u2018not directed at any third party\u2019", "summary": "", "link": "https://theprint.in/diplomacy/beijing-hosts-1st-pakistan-china-bangladesh-trilateral-says-not-directed-at-any-third-party/2664820/", "author": "None"}, {"title": "Operation Sindhu: India To Evacuate Citizens Of Nepal, Sri Lanka From Conflict-Torn Iran", "summary": "", "link": "https://www.news18.com/world/operation-sindhu-india-to-evacuate-citizens-of-nepal-sri-lanka-from-conflict-torn-iran-9396553.html", "author": "None"}, {"title": "Court blocks Louisiana law requiring schools to post Ten Commandments in classrooms", "summary": "", "link": "https://apnews.com/article/louisiana-ten-commandments-schools-federal-court-80d31b705fccbbbe3eeeb3cda5f64ec3", "author": "None"}, {"title": "1,000-Year-Old Mummy Discovered During Gas Line Work in Peru", "summary": "", "link": "http://ancientist.com/2025/06/20/1000-year-old-mummy-discovered-during-gas-line-work-in-peru/", "author": "None"}, {"title": "PM Modi concludes visit to Croatia after meeting top leadership", "summary": "", "link": "https://www.thehindu.com/news/national/pm-modi-concludes-visit-to-croatia-after-meeting-top-leadership/article69711620.ece", "author": "None"}, {"title": "Researchers found that plastic bag bans led to a 25% to 47% decrease in plastic bags as a share of total items collected in shoreline cleanups relative to areas without policies. This decrease grows in magnitude over time, with no evidence of the rates rebounding.", "summary": "", "link": "https://www.bbc.co.uk/news/articles/cdeznkxyek0o", "author": "None"}]}}, "Sport": {}};